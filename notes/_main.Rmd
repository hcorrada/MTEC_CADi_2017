---
title: "CADI 2017"
author: "Héctor Corrada Bravo"
date: "`r Sys.Date()`"
output: 
  bookdown::gitbook
---

# Preamble

Here is where the course notes will be included.

<!--chapter:end:index.rmd-->

# Introduction and Overview

## What is Data Science?

Data science encapsulates the interdisciplinary activities required to create data-centric artifacts and applications that address specific scientific, socio-political, business, or other questions.

Let's look at the constiuent parts of this statement:

### Data

Measureable units of information gathered or captured from activity of people, places and things.

### Specific Questions

Seeking to understand a phenomenon, natural, social or other, can we formulate specific questions for which an answer posed in terms of patterns observed, tested and or modeled in data is appropriate.

### Interdisciplinary Activities

Formulating a question, assessing the appropriateness of the data and findings used to find an answer require understanding of the specific subject area. Deciding on the appropriateness of models and inferences made from models based on the data at hand requires understanding of statistical and computational methods.

### Data-centric artifacts and applications

Answers to questions derived from data are usually shared and published in meaningful, succint but sufficient, reproducible artifacts (papers, books, movies, comics). Going a step further, interactive applications that let others explore data, models and inferences are great.

## Why Data Science?

The granularity, size and accessibility data, comprising both physical, social, commercial and political spheres has exploded in the last decade or more. 

> I keep saying that the sexy job in the next 10 years will be statisticians”

> Hal Varian, Chief Economist at Google (http://www.nytimes.com/2009/08/06/technology/06stats.html?_r=0)

> “The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids.”

> “Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it.”

> Hal Varian
(http://www.mckinsey.com/insights/innovation/hal_varian_on_how_the_web_challenges_managers)

## Data Science in Humanities and Social Sciences

Because of the large amount of data produced across many
spheres of human social and creative activity, many questions in the humanities and social sciences may be addressed by establishing patterns in data. In the humanities, this can range from unproblematic quesitions of how to dissect a large creative corpora, say music, literature, based on raw characteristics of those works, text, sound and image. To more problematic questions, of analysis of intent, understanding, appreciation and valuation of these creative corpora.

In the social sciences, issues of fairness and transparency in the current era of big data are especially problematic. Is data collected representative of population for which inferences are drawn? Are methods employed learning latent unfair factors from ostensibly fair data? These are issues that the research community is now starting to address.

In all settings, issues of ethical collection of data, application of models, and deployment of data-centric artifacts are essential to grapple with. Issues of privacy are equally important.

## Course organization

This course will cover basics of how to use the R data analysis environment for data science activities in humanities and social science. The general organization of the course will be the following:

### Day 1: Introduction and preparation

- Get ourselved settled with R
- Formulating a question that can be addressed with data
- How to get data to address a specific question
- How to manipulate this data to get to what we need for our question of interest
- How to visualize this data 

### Day 2: Use cases

- Social media analysis: text and network structures
- Fitting and evaluating regression models in R
- Text analysis
- Machine Learning modeling in R
- Survey analysis 
- Nested models in R

### Day 3: Presentation and teaching

- How to present and publish analysis using R
- Using R in teaching

## General Workflow

The data science activities we will cover are roughly organized into a general workflow that will help us navigate this material.

![](img/zumel_mount_cycle.png)

### Defining the goal

- What is the question/problem?
- Who wants to answer/solve it?
- What do they know/do now?
- How well can we expect to answer/solve it?
- How well do they want us to answer/solve it?

### Data collection and Management

- What data is available?
- Is it good enough?
- Is it enough?
- What are sensible measurements to derive from this data?
  Units, transformations, rates, ratios, etc.
  
### Modeling

- What kind of problem is it?
  E.g., classification, clustering, regression, etc.
- What kind of model should I use?
- Do I have enough data for it?
- Does it really answer the question?

### Model evaluation

- Did it work? How well?
- Can I interpret the model?
- What have I learned?

### Presentation

- Again, what are the measurements that tell the real story?
- How can I describe and visualize them effectively?

### Deployment

- Where will it be hosted? 
- Who will use it?
- Who will maintain it?


<!--chapter:end:01-intro.rmd-->

# An Illustrative Analysis

http://fivethirtyeight.com has a clever series of articles on the types of movies different actors make in their careers: https://fivethirtyeight.com/tag/hollywood-taxonomy/

I'd like to do a similar analysis. Let's do this in order:

1) Let's do this analysis for Diego Luna 
2) Let's use a clustering algorithm to determine the different types of movies they make
3) Then, let's write an application that performs this analysis for any actor and test it with Gael García Bernal
4) Let's make the application interactive so that a user can change the actor and the number of movie clusters the method learns.

For now, we will go step by step through this
analysis without showing how we perform this analysis using R. As the course progresses, we will learn how to carry out these steps.

## Gathering data

### Movie ratings

For this analysis we need to get the movies Diego Luna was in, along with their Rotten Tomatoes ratings. For that we scrape this webpage: https://www.rottentomatoes.com/celebrity/diego_luna.

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(rvest)
library(stringr)
```

```{r read_dl, echo=FALSE, cache=TRUE, message=FALSE}
# URL base for search
base_url <- "https://www.rottentomatoes.com/celebrity/"

# let's see how this works for Diego Luna

# scrape the table from the website
dl_url <- paste0(base_url, "diego_luna")
dl_html <- read_html(dl_url) 
dl_tab <-  dl_html %>%
  html_node("#filmographyTbl") %>%
  html_table() %>%
  as_tibble()

# clean it up
clean_dl_tab <- dl_tab %>% 
  # make sure the movie is rated
  filter(RATING != "No Score Yet") %>% 
  
  # make the rating look numeric
  mutate(RATING = str_replace(RATING, "%", "")) %>%
  
  # remove producer and director credits
  filter(!str_detect(CREDIT, "Prod") &
         !str_detect(CREDIT, "Dir")) %>%
  
  # convert to proper types
  readr::type_convert()
```

Once we scrape the data from the Rotten Tomatoes website and clean it up, this is part of what we have so far:

```{r, echo=FALSE}
clean_dl_tab %>% head(7) %>% knitr::kable()
```

This data includes, for each of the movies Diego Luna has acted in, the rotten tomatoes rating, the movie title, Diego Luna's role in the movie, the U.S. domestic gross and the year of release.

### Movie budgets and revenue

For the movie budgets and revenue data we scrape this
webpage: http://www.the-numbers.com/movie/budgets/all

```{r read_budget, echo=FALSE, cache=TRUE}
# scrape the webpage
budget_url <- "http://www.the-numbers.com/movie/budgets/all"
budget_html <- read_html(budget_url)
budget_tab <- budget_html %>%
  html_node("table") %>%
  html_table(fill=TRUE) %>%
  select(-1) %>%
  as_tibble()
```

```{r, echo=FALSE}
# clean up the result
clean_budget_tab <- budget_tab %>%
  # remove all those NA rows
  filter(!is.na(`Release Date`)) %>%

  # make the budget columns look numeric 
  mutate_at(vars(-1,-2), funs(str_replace(., "\\$", ""))) %>%
  mutate_at(vars(-1,-2), funs(str_replace_all(., ",", ""))) %>%
  
  # rename columns
  rename(release_date=`Release Date`,
         movie=Movie,
         production_budget=`Production Budget`,
         domestic_gross=`Domestic Gross`,
         worldwide_gross=`Worldwide Gross`) %>%
  
  # convert columns to proper types
  type_convert(cols(release_date=col_date(format="%m/%d/%Y"))) %>%
  
  # represent budget and gross in millions
  mutate_at(vars(-1,-2), funs(. / 1e6))
```

This is part of what we have for that table after scraping and cleaning up:

```{r, echo=FALSE}
clean_budget_tab %>% head(10) %>% knitr::kable()
```

This data is for `r nrow(clean_budget_tab)` movies, including its release date, title, production budget,
US domestic and worlwide gross earnings. The latter three are in millions of U.S. dollars.

One thing we might want to check is if the budget and gross entries in this table are inflation adjusted or not. To do this, we can make a plot of domestic gross, which we are using for the subsequent analyses.

```{r, echo=FALSE}
library(lubridate)

clean_budget_tab %>%
  mutate(year=factor(year(release_date))) %>%
  ggplot() +
  aes(x=year, y=domestic_gross) +
  geom_boxplot() +
  theme_bw()
```

Although we don't know for sure, since the source of our data does not state this specifically, it looks like the domestic gross measurement is inflation adjusted since average gross is stable across years.

## Manipulating the data

Next, we combine the datasets we obtained to get closer to the data we need to make the plot we want.

```{r, echo=FALSE}
joined_tab <- clean_dl_tab %>%
  # join the two tables together
  inner_join(clean_budget_tab, by=c(TITLE="movie")) 
```

We combine the two datasets using the movie title, so
that the end result has the information in both tables for each movie.

```{r, echo=FALSE}
joined_tab %>% knitr::kable()
```

## Visualizing the data

Now that we have the data we need, we can make a plot:

```{r, echo=FALSE, fig.cap="Ratings and U.S. Domestic Gross of Diego Luna's movies."}
joined_tab %>%
  ggplot() +
    theme_bw() +
    aes(x=RATING, y=domestic_gross) +
    geom_point() +
    labs(title="Diego Luna's movies",
         x="Rotten Tomato Rating",
         y="Domestic gross (Millions)")
```

We see that there is one clear outlier in Diego Luna's movies, which probably is the one Star Wars movie he acted in. The remaining movies could potentially be grouped into two types of movies, those with higher rating and those with lower ratings.

## Modeling data

We can use a clustering algorithm to partition Diego Luna's movies. We can use the data we obtained so far and see if the k-means clustering algorithm partitions these movies into three sensible groups using the movie's rating and domestic gross.

```{r, echo=FALSE}
library(class)
library(broom)

kmeans_result <- joined_tab %>%
  select(RATING, domestic_gross) %>%
  kmeans(centers=3) 

clustered_tab <- kmeans_result %>%
  augment(data=joined_tab) %>%
  rename(cluster=.cluster) %>%
  as_tibble()

kmeans_centers <- kmeans_result %>%
  tidy() %>%
  as_tibble()
```

Let's see how the movies are grouped:

```{r, echo=FALSE}
clustered_tab %>%
  select(TITLE, RATING, domestic_gross, cluster) %>%
  arrange(cluster) %>%
  knitr::kable()
```

## Visualizing model result

Let's remake the same plot as before, but use color to
indicate each movie's cluster assignment given by the k-means algorithm.

```{r, echo=FALSE}
final_plot <- clustered_tab %>%
  ggplot() +
    aes(x=RATING, y=domestic_gross, color=cluster) +
    geom_point(size=2.3) +
    theme_bw() +
    labs(title="Diego Luna's movies",
         x="Rotten Tomatoes rating",
         y="Domestic Gross (Millions)")
final_plot
```

The algorithm did make the Star Wars movie it's own group since it's so different that the other movies. The grouping of the remaining movies is not as clean.

To make the plot and clustering more interpretable, let's annotate the graph with some movie titles. In the k-means algorithm, each group of movies is represented by an average rating and an average domestic gross. What we can do is find the movie in each group that is closest to the average and use that movie title to annotate each group in the plot.

```{r, echo=FALSE, message=FALSE}
# join the extended movie table with the centers table
annot_tab <- clustered_tab %>%
  select(title=TITLE, rating=RATING, domestic_gross, cluster) %>%
  left_join(select(kmeans_centers, x1, x2, cluster)) %>%
  
  # calculate the distance of each movie to its center
  mutate(center_dist=sqrt((rating-x1)^2+(domestic_gross-x2)^2)) %>%
  
  # find the movie closest to each center
  group_by(cluster) %>%
  arrange(center_dist) %>%
  slice(1)
```

```{r, echo=FALSE}
final_plot +
  annotate("text", 
           x=annot_tab$x1,
           y=annot_tab$x2,
           label=annot_tab$title)
```

Roughly, movies are clustered into Star Wars and low vs. high rated movies. The latter seem to have some difference in domestic gross. For example, movies like ["The Terminal"](https://www.rottentomatoes.com/m/1133499_1133499_terminal) have lower rating but make slightly more money than movies like ["Frida"](https://www.rottentomatoes.com/m/frida). We could use statistical modeling to see if that's the case, but will skip that for now. Do note also, that the clustering algorithm we used seems to be assigning one of the movies incorrectly, which warrants further investigation.

## Abstracting the analysis

While not a tremendous success, we decide we want to carry on with this analysis. We would like to do this for other actors' movies. One of the big advantages of using R is that we can write a piece of code that takes an actor's name as input, and reproduces the steps of this analysis for that actor. We call these functions, we'll see them and use them a lot in this course. 

For our analysis, this function must do the following:

1. Scrape movie ratings from Rotten Tomatoes 
2. Clean up the scraped data
3. Join with the budget data we downloaded previously
4. Perform the clustering algorithm
5. Make the final plot

With this in mind, we can write functions for each of these steps, and then make one final function that puts all of these together.

For instance, let's write the scraping function. It will take an actor's name and output the scraped data.

```{r, echo=FALSE}
scrape_rt <- function(actor, base_url="https://www.rottentomatoes.com/celebrity/") {
  url <- paste0(base_url, actor)
  html <- read_html(url) 
    
  html %>%
    html_nodes("#filmographyTbl") %>%
    html_table() %>%
    magrittr::extract2(1) %>%
    as_tibble()
}
```

Let's test it with Gael García Bernal:

```{r scrape_ggb, cache=FALSE, echo=FALSE}
ggb_tab <- scrape_rt("gael_garcia_bernal")
```

```{r, echo=FALSE}
ggb_tab %>%
  head(3) %>%
  knitr::kable()
```

Good start. We can then write functions for each of the steps we did with Diego Luna before.

```{r, echo=FALSE}
cleanup_rt_tab <- function(data) {
 data  %>% 
  # make sure the movie is rated
  filter(RATING != "No Score Yet") %>% 
  
  # make the rating look numeric
  mutate(RATING = str_replace(RATING, "%", "")) %>%
  
  # remove producer and director credits
  filter(!str_detect(CREDIT, "Prod") &
         !str_detect(CREDIT, "Dir")) %>%
  
  # convert to proper types
  readr::type_convert()
}
```

```{r, echo=FALSE}
join_budget <- function(data) {
  data %>%
  # join the two tables together
  inner_join(clean_budget_tab, by=c(TITLE="movie")) 
}
```

```{r, echo=FALSE, eval=FALSE}
ggb_tab %>%
  cleanup_rt_tab() %>%
  join_budget() %>%
  head() %>%
  knitr::kable()
```

```{r, echo=FALSE}
cluster_movies <- function(data, k=3) {
  data <- data %>%
    select(rating=RATING, title=TITLE, domestic_gross)
  
  kmeans_result <-  data %>%
    select(rating, domestic_gross) %>%
    kmeans(centers=k) 

  clustered_tab <- kmeans_result %>%
    augment(data=data) %>%
    rename(cluster=.cluster) %>%
    as_tibble()

  kmeans_centers <- kmeans_result %>%
    tidy() %>%
    as_tibble()

   clustered_tab %>%
    left_join(select(kmeans_centers, x1, x2, cluster)) %>%
  
    # calculate the distance of each movie to its center
    mutate(center_dist=sqrt((rating-x1)^2+(domestic_gross-x2)^2))
}
```

```{r, echo=FALSE, eval=FALSE}
ggb_tab %>%
  cleanup_rt_tab() %>%
  join_budget() %>%
  cluster_movies() %>%
  knitr::kable()
```

```{r, echo=FALSE}
plot_movies <- function(data, actor) {
  plt <- data %>% ggplot() +
    aes(x=rating, y=domestic_gross, color=cluster) +
    geom_point(size=2.3) +
    theme_bw() +
    labs(title=paste0(actor, "'s movies"),
         x="Rotten Tomatoes rating",
         y="Domestic Gross (Millions)")
  
  annot_dat <- data %>%
    group_by(cluster) %>%
    arrange(center_dist) %>%
    slice(1)
  
  plt <- plt +
     annotate("text", 
           x=annot_dat$x1,
           y=annot_dat$x2,
           label=annot_dat$title)
  plt
}
```

```{r, echo=FALSE, eval=FALSE}
ggb_tab %>%
  cleanup_rt_tab() %>%
  join_budget() %>%
  cluster_movies() %>%
  plot_movies("Gael García Bernal")
```

Then put all of these steps into one function that calls our new functions to put all of our analysis together:

```{r, echo=FALSE}
analyze_actor <- function(actor, k=3, base_url="https://www.rottentomatoes.com/celebrity/") {
  # first let's make the name work with RT
  rt_name <- actor %>%
    str_to_lower() %>%
    str_replace_all(" ", "_")
  
  message("Scraping Rotten Tomatoes with name ", rt_name)
  dirty_dat <- scrape_rt(rt_name, base_url=base_url) 
  
  message("Preparing data for analysis")
  clean_dat <- dirty_dat %>%
    cleanup_rt_tab() %>%
    join_budget() 
    
  message("Performing clustering and plotting")
    clean_dat %>% cluster_movies(k=k) %>%
    plot_movies(actor)
}
```

We can test this with Gael García Bernal

```{r test_bbg, cache=TRUE, message=FALSE}
analyze_actor("Gael Garcia Bernal")
```

## Making analyses accessible

Now that we have written a function to analyze an
actor's movies, we can make these analyses easier to produce by creating an interactive application that wraps our new function. The `shiny` R package makes creating this type of application easy. 

```{r movie_app, echo=FALSE, cache=TRUE}
knitr::include_app("https://hcorrada.shinyapps.io/movie_app/")
```

## Summary

In this analysis we saw examples of the common steps and operations in a data analysis:

1) Data ingestion: we scraped and cleaned data from publicly accessible sites 

2) Data manipulation: we integrated data from multiple sources to prepare our analysis

3) Data visualization: we made plots to explore patterns in our data

4) Data modeling: we made a model to capture the grouping patterns in data automatically, using visualization to explore the results of this modeling

5) Publishing: we abstracted our analysis into an application that allows us and others to perform this analysis over more datasets and explore the result of modeling using a variety of parameters



<!--chapter:end:02-first_analysis.rmd-->

# Setting up R

Here we setup R, RStudio and anything else
we will use in the course.


## Setting up R

R is a free, open source, environment for data analysis. It is available as a free binary download for Mac, Linux and Windows. For the more adventorous, it can also be compiled from source. To install R in your computer go to
https://cran.r-project.org/index.html and download and install the appropriate binary file.

![](img/cran.png)

This will install the base R system: the R programming language, a few packages for common data analyses and a development environment. 

## Setting up Rstudio

We will actually use Rstudio to interact with R. Rstudio is a very powerful application to make data analysis with R easier to do. To install go to https://www.rstudio.com/products/rstudio/download/ and download the appropriate version of Rstudio.

![](img/rstudio.png)

## A first look at Rstudio

Let's take a first look at Rstudio. The first thing you will notice is that Rstudio is divided into panes. Let's take a look first at the *Console*.

### Interactive Console

The most immediate way to interact with R is through the interactive console. Here we can write R instructions to perform our data analyses. We want to start using data so the first instructions we will look at deal with loading data.

When you installed R, a few illustrative datasets were installed as well. Let's take a look at the list of datasets you now have access to. Write the following command in the console

```{r, echo=FALSE, eval=FALSE}
data()
```

![](img/rstudio_data.png)

This will list names and descriptions of datasets available in your R installation. Let's try to find out more information about these datasets. In R, the first attempt to get help with something is to use the `?` operation. So, to get help about the `swiss` dataset we can enter the following in the console

```{r, echo=FALSE, eval=FALSE}
?swiss
```

This will make the documentation for the `swiss` dataset open in another pane.

![](img/rstudio_swiss.png)

**On your own:** Find more information about a different dataset using the `?` operator.

### Data Viewer

According to the documentation we just saw for `swiss`, this is a `data.frame` with 47 observations and 6 variables. The `data.frame` is the basic structure we will use to represent data throughout the course. We will see this again repeatedly, and use a couple of other names (e.g., `tibble`) to refer to this. Intuitively, you can think of the `data.frame` like a spreadsheet, with rows representing observations, and columns representing variables that describe those observations. Let's see what the `swiss` data looks like using the Rstudio data viewer.

```{r, echo=FALSE, eval=FALSE}
View(swiss)
```

![](img/rstudio_view_swiss.png)

The Data Viewer lets you reorder data by the values in a column. It also lets you filter rows of the data by values as well.

**On your own**: Use the Data Viewer to explore another of the datasets you saw listed before.

### Names, values and functions

Let's make a very short pause to talk about something you may have noticed. In the console, we've now written a few instructions, e.g. `View(swiss)`. Let's take a closer look at how these instructions are put together.

_expressions_: first of all, we call these instructions _expressions_, which are just text that R can evaluate into a value. `View(swiss)` is an expression.

_values_: so, what's a value? They are numbers, strings, data frames, etc. This is the data we will be working with. The number `2` is a value. So is the string `"Hector"`. 

So, what value is produced when R evaluates the expression `View(swiss)`? Nothing, which we also treat as a value. That wasn't very interesting, but it does have a side effect: it shows the `swiss` dataset in the Data viewer. 

How about a simpler expression: `swiss`, what value is produced when R evaluates the expression `swiss`? The data.frame containing that data. Try it out in the console.

_names_: so if `swiss` isn't a value, what is it? It is a _name_. We use these to refer to values. So, when we write the expression `swiss`, we tell R we want the _value_ referenced by the name `swiss`, that is, the data itself!

![](img/names_values.png)

_functions_: Besides numbers, strings, data frames, etc. another important type of value is the _function_. Functions are a series of instructions that take some input value and produce a different value. The name `View` refers to the function that takes a data frame as input, and displays it in the Data viewer. Functions are called using the parentheses we saw before: `View(swiss)`, the parentheses say that you are passing input `swiss` to the function `View`. We'll see later how we can write our own functions.

### Plotting

Next, I want to show the _Plots_ pane in Rstudio. Let's make a plot using the `swiss` dataset:

```{R, echo=FALSE, eval=FALSE}
plot(swiss$Education, swiss$Fertility)
```

![](img/rstudio_plot_swiss.png)

It's not pretty, but it was very easy to produce. There's a couple of things going on here...

- `plot` is a function, it takes two inputs, the data to put in the x and y axes, evaluates to nothing, but creates a plot of the data

- `swiss$Education` is how we refer to the `Education` column in the `swiss` data frame.

**On your own**: Make a plot using other variables in the `swiss` dataset. 

### Editor

So far, we've made some good progress: we know how to write expressions on the R console so that they are evaluated, we are starting to get a basic understanding of how these expressions are constructed, we can use the Data viewer to explore data frames, and made one plot that was displayed in the Plots pane. To finish this quick tour, I want to look at two more Rstudio panes: the file editor, and the File viewer.

As you have noticed, everytime we want to evaluate an expression on the console, we have to write it in. For example, if we want to change the plot we made above to include a different variable, we have to write the whole thing again. Also, what if I forgot what expression I used to make a specific plot? Even better, what if I wanted somebody else to make the plot I just made?

By far, one of the biggest advantages of using R over Excel or other similar programs, is that we can write expressions in scripts that are easy to share with others, making analyses easier to reproduce. Let's write a script that we can use to make the same plot we just made.

In the Rstudio menu select `File>New File>R Script`

![](img/rstudio_new_script.png)

This will open a tab in the File editor in which we can write expressions:

![](img/rstudio_file.png)

We can then evaluate the expressions in the file one at a time, or all at the same time.

We can then save these expressions in a script. In the Rstudio menu select `File>Save` and save as a text file. The convention is to use the `.R` or `.r` file extension, e.g., `swiss_plot.r`.

**On your own:** Add expressions for additional plots to the script and save again. Run the new expressions.

### Files viewer

Rstudio includes a Files viewer that you can use to find and load files. You can find the Files near the Plots viewer

![](img/rstudio_files.png)

## R packages

Another of R's advantages for data analysis is that it has attracted a large number of extremely useful additions provided by users worldwide. These are housed in [CRAN](https://cran.r-project.org/web/packages/index.html).

In this course we will make a lot of use of a set of packages bundled together into the `tidyverse` by Hadley Wickham and others. These packages make preparing, modeling and visualizing certain kinds data (which covers the vast majority of use cases) quite fun and pleasent. There is a webpage for the general tidyverse project: http://tidyverse.org, which includes pages for each of the packages included there. 

Let's install the `tidyverse` into your R environment. There are two ways of installing packages. In the console, you can use the expression:

```{r, echo=FALSE, eval=FALSE}
install.packages("tidyverse")
```

In Rstudio, you can use the _Packages_ tab:

![](img/rstudio_install_packages.png)

**On your own:** Install the following additional packages which we will use later on: `rvest`, `stringr`, `nycflights13` and `broom`.

## Finishing your setup

Go to the Google form as instructed and complete your exit ticket.





<!--chapter:end:03-setting_up.rmd-->

# R Principles

Now that we have our tools ready, let's start doing some analysis. First, let's go over some principles of R as a data analysis environment. R is a computational environment for data analysis. It is designed around a _functional_ language, as opposed to _procedural_ languages like Java or C, that has desirable properties for the type of operations and workflows that are frequently performed in the course of analyzing datasets. In this exercise we will start learning some of those desirable properties while performing an analysis of a real dataset.

## Some history

R is an offspring of S, a language created in AT&T Labs by John Chambers (now at Stanford) and others in 1976 with the goal of creating an environment for statistical computing and data analysis. The standard for the language in current use was settled in 1998. That same year, "S" won the ACM Software System award, awarded to software systems "that have a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both".

In 1991, Robert Gentleman and Ross Ihaka created R to provide an open source implementation of the S language and environment. They also redesigned the language to enforce lexical scoping rules. It has been maintained by the R core group since 1997, and in 2015 an R consortium, including Microsoft, Google, and others, was created.

Along with Python it is one of the most popular environments for data analysis (e.g., figure below from [KDNuggets 2016 software survey](http://www.kdnuggets.com/2016/06/r-python-top-analytics-data-mining-data-science-software.html)) 

![](img/kdnuggets-2016.jpg)

We use it for this class because we find that besides it being a state-of-the-art data analysis environment, it provides a clean end-to-end platform for teaching material across the data management-modeling-communication spectrum that we study in class. 

## Additional R resources

Resources for learning and reading about R are listed in our [here](http://www.hcbravo.org/IntroDataScience/resources/). Of note are the [swirl project](http://swirlstats.com/) and DataCamp's [introduction to R] course.

One of the biggest strengths of the R ecosystem is the variety and quality of packages for data analysis available. R uses a package system (like Python and Ruby for instance). Packages are divided into two classes: **base** which are packages installed when R is installed, includes packages for basic statistics, computing with probability distributions, plotting and graphics, matrix manipulations and other), all other packages are available in [CRAN](http://cran.r-project.org). We will be using a fair number of these packages through the course of the semester.

## Literate Programming

One last note before we get started. R has great support for [literate programming](http://en.wikipedia.org/wiki/Literate_programming), where source code that contains both code, the result of evaluating that code, and text explaining that code co-exist in a single document. This is extremely valuable in data analysis, as many choices made by data analysts are worth explaning in text, and interpretation of the results of analyses can co-exist with the computations used in that analysis. This document you are reading contains both text and code. In class, we will use [Rmarkdown](http://rmarkdown.rstudio.com/) for this purpose.

## A data analysis to get us going

I'm going to do a very simple analysis of Baltimore crime to show off R. We'll use data downloaded from Baltimore City's awesome open data site (this was downloaded a couple of years ago so if you download now, you will get different results). 

The repository for this particular data is here. [https://data.baltimorecity.gov/Crime/BPD-Arrests/3i3v-ibrt](https://data.baltimorecity.gov/Crime/BPD-Arrests/3i3v-ibrt) 

## Getting data

We've prepared the data previously into a comma-separated value file (`.csv` file). In this format, each line contains _attribute_ values (separated by commas) for one _entity_ in our dataset. Which we can download and load into our R environment.

The `read_csv` command is part of the `readr` R package and allows you to read a dataset stored in a csv file. This function is extremely versatile, and you can read more about it by using the standard help system in R: `?read_csv`. Now, the result of running calling this function is the data itself, so, by running the function in the console, the result of the function is printed. 

## Variables and Value

To make use of this dataset we want to assign the result of calling `read.csv` (i.e., the dataset) to a variable:

```{r vars1}
library(tidyverse)
arrest_tab <- read_csv("data/BPD_Arrests.csv")
```

```{r echo=FALSE}
arrest_tab$race <- factor(arrest_tab$race)
arrest_tab$sex <- factor(arrest_tab$sex)
arrest_tab$incidentOffense <- factor(arrest_tab$incidentOffense)
```
Now we can ask what _type_ of value is stored in the `arrest_tab` variable:

```{r type}
class(arrest_tab)
```

The `data.frame` is a workhorse data structure in R. It encapsulates the idea of _entities_ (in rows) and _attribute values_ (in columns). We can ask other features of this dataset:

```{r questions}
# This is a comment in R, by the way

# How many rows (entities) does this dataset contain?
nrow(arrest_tab)

# How many columns (attributes)?
ncol(arrest_tab)

# What are the names of those columns?
colnames(arrest_tab)
```

Now, in Rstudio you can view the data frame using `View(arrest_tab)`.

## Indexing

A basic operation in data analysis is selecting subsets of a dataset. For that we can use a few alternative options for _indexing_ into datasets.

```{r}
# to obtain the value in the first row, fifth column:
arrest_tab[1,5]

# note that indexing in R is 1-based, not 0-based, so the first row is indexed by 1

# now we want to do a bit more, so let's say we want the value in the fifth column of our dataset for the first 10 rows. For that we can use slice notation:
arrest_tab[1:10,5]

# similarly, to obtain the value in the first five columns of the first row
arrest_tab[1,1:5]

# what is the class of the value when we subset a single column?
class(arrest_tab[1:10,5])

# what is the class of the value when we subset a single row?
class(arrest_tab[1,1:5])

# what do we get with this indexing?
arrest_tab[1:10,1:5]
```

We can index any set of rows or columns by constructing _vectors_ of integers. In fact, the slice notation `:` is essentially doing that for a sequence of consecutive indices. You should think of vectors as lists of values with the same class.

If we want non-consecutive indices we have other options (e.g., the `c` function, for "concatenate")

```{r}
# non-consecutive indices using c
arrest_tab[c(2,4,7,10), 1:5]

# here's a fun one, when we call columns for a subset of rows
arrest_tab[c(2,4,7,10), ]

# there is also the `seq` function, to create sequences
arrest_tab[seq(from=1,to=10), seq(1,10)]

# that is equivalent to 
arrest_tab[1:10,1:10]

# with the `seq` function you can do more sophisticated things like select only entries in odd rows (1,3,5,7...)
head(arrest_tab[seq(from=1,to=nrow(arrest_tab),by=2), ])
```

Now, since columns have names, we can also use strings (and vectors of strings) to index data frames.

```{r}
# single column
arrest_tab[1:10, "age"]

# multiple columns
arrest_tab[1:10, c("age", "sex", "race")]
```

If we wanted a single named column from a data frame there's a special operator `$` to index:

```{r}
# first ten values of the age column
arrest_tab$age[1:10]

# EXERCISE
# try using three different ways of selecting rows 20 to 30 # of the "sex" column
```

In addition to integer indices or names, we can use vectors of logical values for indexing. 

```{r}
# rows 2,4,7 and 10 using logical indices
arrest_tab[c(FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,rep(FALSE,nrow(arrest_tab)-10)),]

# now here's a fun one, if we only wanted odd rows
head(arrest_tab[c(TRUE,FALSE),])
```

The last example shows one of the most common gotchas in R. Indices are recycled. For instance if selecting rows, if you pass a logical vector that's shorter than the number of rows  in the data frame, the vector will be recycled as many times as necessary to match the number of rows in the dataset. Now, why is this useful, because a pithy index vector can let you select easily. Why is this bad, because errors in code can go easily unnoticed. So in this case, the price of ease of use is paid by the programmer by having to think a lot more carefully about their code (this is a theme in R programming...)

The utility of logical indexing is that now we can select rows based on a property of its values for a given column

```{r}
# select rows for entities younger than 21 years old
head(arrest_tab[arrest_tab$age < 21, ])

# notice that the value of expression `arrest_tab$age < 21` # is a logical vector

# select entities (arrests) occuring in Mount Washington,
# a specific neighborhood in Baltimore
head(arrest_tab[arrest_tab$neighborhood == "Mount Washington",])

# how about arrests where subjects are under 21 in Mount  Washington? 
# use a logical `and` operator
indices <- arrest_tab$age < 21 & arrest_tab$neighborhood == "Mount Washington"
```


## Exploration

R has built-in functions that help easily obtain summary information about datasets. For instance:

```{r}
summary(arrest_tab$sex)
summary(arrest_tab$race)

# well that seems problematic
# let's rename columns to correct that
colnames(arrest_tab)[3:4] <- c("race", "sex")
```

We can also ask other useful type of summaries

```{r}
# What is the average age in arrests?
mean(arrest_tab$age)

# Median age?
median(arrest_tab$age)

# what types of offenses are there
summary(arrest_tab$incidentOffense)

# what does summary looks like for continuous attributes?
summary(arrest_tab$age)
```

Combining this type of summary with our indexing strategies we learned previously we can ask more specific questions

```{r}
# What is the average age for arrests in Mount Washington?
mount_washington_index <- arrest_tab$neighborhood == "Mount Washington"

mean(arrest_tab$age[mount_washington_index], na.rm=TRUE)

# How about the number of arrests in Mount Washington _stratified_ by race and sex?
table(arrest_tab$race[mount_washington_index], arrest_tab$sex[mount_washington_index])

# how about a graphical summary of arrest ages in Mount Washington?
# we'll use a boxplot
boxplot(arrest_tab$age[mount_washington_index])

# can we do the same stratified by sex?
boxplot(arrest_tab$age[mount_washington_index]~arrest_tab$sex[mount_washington_index])
```

This used a very useful notation in R: the tilde, `~` which we will encounter in a few different places. One way of thinking about that abstractly is, do something with this attribute, as a function (or depending on, stratified by, conditioned on) this other attribute. For instance, "plot `age` as a function of sex" in our example.

Let's write code that's a little cleaner for that last plot,
 and let's also make the plot a bit more useful by adding a title and axis labels:
 
```{r}
mount_washington_tab <- arrest_tab[mount_washington_index,]
boxplot(mount_washington_tab$age~mount_washington_tab$sex,
        main="Mt. Washington", 
        xlab="Sex", ylab="Arrest Age")
```

Here's one more useful plot:

```{r}
barplot(table(mount_washington_tab$race), 
        xlab="Number of Arrests",
        ylab="Race")
```

## Functions

Now suppose we wanted to do a similar analysis for other neighborhoods. In that case we should encapsulate the summaries and plots we want to do in a function:

```{r}
analyze_neighborhood <- function(neighborhood) {
  neighborhood_index <- arrest_tab$neighborhood == neighborhood
  neighborhood_tab <- arrest_tab[neighborhood_index,]
  
    boxplot(neighborhood_tab$age~neighborhood_tab$sex,
            main = neighborhood,
            xlab = "Sex", ylab="Arrest Age")
    
    barplot(table(neighborhood_tab$race),
            main = neighborhood,
            xlab = "Race", ylab="Number of Arrests")
}
```

Now we can use that function to make our plots for specific neighborhoods

```{r}
analyze_neighborhood("Mount Washington")
analyze_neighborhood("Hampden")
```

## A note on data types

This dataset contains data of types commonly found in data analyses

- Numeric (continuous): A numeric measurement (e.g., height)  
- Numeric (discrete): Usually obtained from counting, think only integers (e.g., `age` which is measured in years)  
- Categorical: One of a possible set of values (e.g., `sex`)  
- Datetime: Date and time of some event or observation (e.g., `arrestDate`, `arrestTime`)  
- geolocation: Latitude and Longitude of some event or observation (e.g., `Location.`)  

The distinction between continuous and discrete is a bit tricky since measurements that have finite precision must be discrete. So, the difference really comes up when we build statistical models of datasets for analysis. For now, think of discrete data as the result of counting, and continuous data the result of some physical measurement.

We said that R is designed for data analysis. My favorite example of how that manifests itself is the `factor` datatype. If you look at your dataset now, `arrest_tab$sex` is a vector of strings:

```{r}
class(arrest_tab$sex)
summary(arrest_tab$sex)
```

However, as a measurement, or attribute, it should only take one of two values (or three depending on how you record missing, unknown or unspecified). So, in R, that categorical data type is called a _factor_. Notice what the `summary` function does after turning the `sex` attribute into a _factor_:

```{r}
arrest_tab$sex <- factor(arrest_tab$sex)
summary(arrest_tab$sex)
```

This distinction shows up in many other places where functions have very different behavior when called on a vector of strings and when called on a factor (e.g., functions that make plots, or functions that learn statistical models).

One last note, the possible values a _factor_ can take are called _levels_:

```{r}
levels(arrest_tab$sex)
```

Exercise: you should transform the `race` attribute into a factor as well. How many levels does it have?

## Thinking in vectors

In data analysis the _vector_ is probably the most fundamental data type (other than basic numbers, strings, etc.). Why? Consider getting data about one attribute, say height, for a group of people. What do you get, an array of numbers, all in the same unit (say feet, inches or centimeters). How about their name? Then you get an array of strings. Abstractly, we think of vectors as arrays of values, all of the same _class_ or datatype. 

In our dataset, each column, corresponding to an attribute, is a vector:

```{r}
# the 'str' function gives a bit more low-level information about objects
str(arrest_tab$Location)
```

R (and other data analysis languages) are designed to operate on vectors easily. For example, frequently we want to do some kind of transformation to a data attribute, say record age in months rather than years. Then we would perform the **same operation** for every value in the corresponding vector:

```{r}
age_in_months <- arrest_tab$age * 12
```

In a language that doesn't support this type of vectorized operation, you would use a loop, or similar construct, to perform this operation.

Another type of transformation frequently done is to combine attributes into a single attribute. Suppose we wanted to combine the `arrestLocation` and `neighborhood` attributes into an `address` attribute:

```{r}
# remember you can always find out what a function does by using ?paste
head(paste(arrest_tab$arrestLocation, arrest_tab$neighborhood, sep=", "))
```

Here the `paste` function concatenates strings element-wise: the first string in `arrestLocation` is concatenated with the first string in `neighborhood`, etc.

Arithmetic operations have the same element-wise operation:

```{r}
# add first 10 odd numbers to first 10 even numbers
seq(1, 20, by=2) + seq(2, 20, by=2)
```

## Lists vs. vectors

We saw that vectors are arrays of values, all of the same _class_. R also allows arrays of values that have different _class_ or datatype. These are called _lists_. Here is a list containing a string, and a couple of numbers:

```{r}
my_list <- list("Hector", 40, 71)
my_list
```

Indexing in lists uses different syntax from the indexing we saw before. To index an element in a list we would use a double-bracket `[[`. 

```{r}
my_list[[1]]
```

In contrast, the single bracket `[` indexes a _part_ of the list, and thus returns another list.

```{r}
my_list[1]
```

That way we can use slice notation and other operations we saw when indexing vectors as before, but we get lists as results.

```{r}
my_list[1:2]
```

List elements can have names as well:

```{r}
named_list <- list(person="Hector", age=40, height=71)
named_list
```

Which we can use to index elements as well (both with `[[` and `$`)

```{r}
named_list[["person"]]
named_list$person
```

Lists can hold arbitrary objects as elements. For example you can have a vector of strings as an element in a list

```{r}
my_list <- list(person=c("Hector", "Ringo", "Paul", "John"), 40, 71)
my_list
```

Now, we come to a momentous occassion in understanding R. `data.frame`s are special instances of _lists_! But, in this case, every element in the list is a vector, and all vectors have exactly the same length. So `arrest_tab$age` indexes the named element `age` in the list `arrest_tab`!

The pattern of _applying_ functions to entries in vectors also holds for elements in lists. So, if we want to calculate smallest value for every attribute in our dataset, we could do something like this:

```{r}
sapply(arrest_tab, function(v) sort(v)[1])
```

## Making the process explicit with pipes

We've discussed the idea of thinking about data analysis work in terms of "pipelines", where
we start from data of a certain shape (e.g., a `data.frame`) and apply transformations (functions) to obtain data that contains the computation we want. Consider the following example seen in class:

_What is the mean age of males arrested in the SOUTHERN district?_

We can frame the answer to this question as a series of data transformations to get the answer we are looking for:

```{r}
# filter data to observations we need
index_vector <- arrest_tab$sex == "M" & arrest_tab$district == "SOUTHERN"
tmp <- arrest_tab[index_vector,]

# select the attribute/column we need
tmp <- tmp[["age"]]

# compute statistic required
mean(tmp, na.rm=TRUE)
```

Let's rewrite this using functions to illustrate the point

```{r}
filter_data <- function(data) {
  index_vector <- data$sex == "M" & data$district == "SOUTHERN"
  data[index_vector,]
}

select_column <- function(data, column) {
  data[[column]]
}

tmp <- filter_data(arrest_tab)
tmp <- select_column(tmp, "age")
mean(tmp, na.rm=TRUE)
```

So, this pattern of _data-->transform-->data_ becomes clearer when written that way.

The `dplyr` package introduces _syntactic sugar_ to make this explicit. We can write the above snippet using the "pipe" operator `%>%`:

```{r}
arrest_tab %>%
  filter_data() %>%
  select_column("age") %>%
  mean(na.rm=TRUE)
```

The `%>%` binary operator takes the value to its **left** and inserts it as the first argument of the function call to its **right**. So the expression `LHS %>% f(another_argument)` is **equivalent** to the expression `f(LHS, another_argument)`. We will see this pattern extensively in class because it explicitly presents the way we want to organize many of our data analysis tasks.

<!--chapter:end:04-r_principles.rmd-->

# Ingesting data

Now that we have a better understanding of the R data analysis language, we turn to the first significant challenge in data analysis, getting data into R in a shape that we can use to start our analysis. We will look at two types of data ingestion: _structured ingestion_, where we read data that is already structured, like a comma separated value (CSV) file, and _scraping_ where we obtain data from text, usually in websites.

## Structured ingestion

### CSV files (and similar)

We saw in a previous chapter how we can use the `read_csv` file to read data from a CSV file into a data frame. Comma separated value (CSV) files are structured in a somewhat regular way, so reading into a data frame is straightforward. Each line in the CSV file corresponds to an observation (a row in a data frame). Each line contains values separated by a comma (`,`), corresponding to the variables of each observation. 

This ideal principle of how a CSV file is constructed is frequently violated by data contained in CSV files. To get a sense of how to deal with these cases look at the documentation of the `read_csv` function. For instance:

- the first line of the file may or may not contain the names of variables for the data frame (`col_names` argument). 

- strings are quoted using `'` instead of `"` (`quote` argument)

- missing data is encoded with a non-standard code, e.g., `-` (`na` argument)

- values are separated by a character other than `,` (`read_delim` function)

- file may contain header information before the actual data so we have to skip some lines when loading the data (`skip` argument)

You should read the documentation of the `read_csv` function to appreciate the complexities it can maneuver when reading data from structured text files.

```{r, eval=FALSE}
?read_csv
```

### Excel spreadsheets

Often you will need to ingest data that is stored in an Excel spreadsheet. The `readxl` package is used to do this. The main function for this package is the `read_excel` function. It contains similar arguments to the `read_csv` function we saw above. 

**On your own:** Use the `read_excel` function to parse migration data from the 2009 INEGI national survey contained in file `data/Migracion_interna_eua.xls`.

## Scraping

Often, data we want to use is hosted as part of HTML files in webpages. The markup structure of HTML allows to parse data into tables we can use for analysis. Let's use the Rotten Tomatoes ratings webpage for Diego Luna as an example:

![](img/rt_diegoluna.png)

We can scrape ratings for his movies from this page. To do this we need to figure out how the HTML page's markup can help us write R expressions to find this data in the page. Most web browsers have facilities to show page markup. In Google Chrome, you can use `View>Developer>Developer Tools`, and inspect the page markdown to find where the data is contained. In this example, we see that the data we want is in a `<table>` element in the page, with id `filmographyTbl`.

![](img/rt_devtools.png)

Now that we have that information, we can use the `rvest` package to scrape this data:

```{r ingest_dl, cache=TRUE}
library(rvest)

url <- "https://www.rottentomatoes.com/celebrity/diego_luna"

dl_tab <- url %>%
  read_html() %>%
  html_node("#filmographyTbl") %>%
  html_table()

head(dl_tab)
```

The main two functions we used here are `html_node` and `html_table`. `html_node` finds elements in the HTML page according to some selection criteria. Since we want the element with `id=filmographyTbl` we use the `#` selection operation since that corresponds to selection by id. Once the desired element in the page is selected, we can use the `html_table` function to parse the element's text into a data frame.

**On your own:** If you wanted to extract the TV filmography from the page, how would you change this call?

**On your own:** We can get movie budget and gross information from this page: http://www.the-numbers.com/movie/budgets/all. Write R code to scrape the budget data from that page.



<!--chapter:end:05-ingesting.rmd-->

# Tidying data

This section is concerned with common problems in data preparation, namely use cases commonly found in raw datasets that need to be addressed to turn messy data into tidy data. These would be operations that you would perform on data obtained as a csv file from a collaborator or data repository, or as the result of scraping data from webpages or other sources. We derive many of our ideas from the paper [Tidy Data](http://www.jstatsoft.org/v59/i10/paper) by Hadley Wickham. Associated with that paper we will use two very powerful R libraries `tidyr` and `dplyr` which are extremely useful in writing scripts for data cleaning, preparation and summarization. A basic design principle behind these libraries is trying to effectively and efficiently capture very common use cases and operations performed in data cleaning. The paper frames these use cases and operations which are them implemented in software.

## Tidy Data

Here we assume we are working with a data model based on rectangular data structures where

1. Each attribute (or variable) forms a column  
2. Each entity (or observation) forms a row  
3. Each type of entity (observational unit) forms a table  

Here is an example of a tidy dataset: 

```{r}
library(nycflights13)
head(flights)
```

it has one observation per row, a single variable per column. Notice only information about flights are included here (e.g., no airport information other than the name) in these observations.

## Common problems in messy data

The set of common operations we will study are based on these common problems found in datasets. We will see each one in detail:

- Column headers are values, not variable names (gather)  
- Multiple variables stored in one column (split)  
- Variables stored in both rows and column (rotate)  
- Multiple types of observational units are stored in the same table (normalize)  
- Single observational unit stored in multiple tables (join)  

We are using data from Hadley's paper found in [github](https://github.com/hadley/tidyr). It's included directory `data`:

```{r, eval=TRUE, echo=TRUE}
data_dir <- "data"
```


### Headers as values

The first problem we'll see is the case where a table header contains values. At this point we will introduce the `dplyr` package, which we'll use extensively in this course. It is an extremely powerful and efficient way of manipulating tidy data. It will serve as the core of our data manipulation knowledge after this course.

`dplyr` defines a slightly different way of using data.frames. The `tbl_df` function converts a standard R data.frame into a `tbl_df` defined by `dplyr`. One nice thing it does, for example, is print tables in a much friendlier way.

```{r}
library(tidyr)
library(dplyr)
library(readr)

pew <- read_csv(file.path(data_dir, "pew.csv"))
pew
```

This table has the number of survey respondents of a specific religion that report their income within some range. A tidy version of this table would consider the *variables* of each observation to be `religion, income, frequency` where `frequency` has the number of respondents for each religion and income range. The function to use in the `tidyr` package is `gather`:

```{r}
tidy_pew <- gather(pew, income, frequency, -religion)
tidy_pew
```

This says: gather all the columns from the `pew` (except `religion`) into key-value columns `income` and `frequency`. This table is much easier to use in other analyses.

Another example: this table has a row for each song appearing in the billboard top 100. It contains track information, and the date it entered the top 100. It then shows the rank in each of the next 76 weeks.

```{r}
billboard <- read_csv(file.path(data_dir, "billboard.csv"))
billboard
```

Challenge:
This dataset has values as column names. Which column names are values? How do we tidy this dataset?

### Multiple variables in one column

The next problem we'll see is the case when we see multiple variables in a single column. Consider the following dataset of tuberculosis cases:

```{r}
tb <- read_csv(file.path(data_dir, "tb.csv"))
tb
```

This table has a row for each year and strain of tuberculosis (given by the first two columns). The remaining columns state the number of cases for a given demographic. For example, `m1524` corresponds to males between 15 and 24 years old, and `f1524` are females age 15-24. As you can see each of these columns has two variables: `sex` and `age`.

Challenge: what else is untidy about this dataset?

So, we have to do two operations to tidy this table, first we need to use `gather` the tabulation columns into a `demo` and `n` columns (for demographic and number of cases):

```{r}
tidy_tb <- gather(tb, demo, n, -iso2, -year)
tidy_tb
```

Next, we need to `separate` the values in the `demo` column into two variables `sex` and `age`

```{r}
tidy_tb <- separate(tidy_tb, demo, c("sex", "age"), sep=1)
tidy_tb
```

This calls the `separate` function on table `tidy_db`, separating the `demo` variable into variables `sex` and `age` by separating each value after the first character (that's the `sep` argument).

We can put these two commands together in a pipeline:

```{r}
tidy_tb <- tb %>% 
  gather(demo, n, -iso2, -year)  %>%
  separate(demo, c("sex", "age"), sep=1)
tidy_tb
```

### Variables stored in both rows and columns

This is the messiest, commonly found type of data. Let's take a look at an example, this is daily weather data from for one weather station in Mexico in 2010.

```{r}
weather <- read_csv(file.path(data_dir, "weather.csv"))
weather
```

So, we have two rows for each month, one with maximum daily temperature, one with minimum daily temperature, the columns starting with `d` correspond to the day in the where the measurements were made.

Challenge: How would a tidy version of this data look like?

```{r}
weather %>%
  gather(day, value, d1:d31, na.rm=TRUE) %>%
  spread(element, value)
```

The new function we've used here is `spread`. It does the inverse of `gather` it spreads columns `element` and `value` into separate columns.

### Multiple types in one table

Remember that an important aspect of tidy data is that it contains exactly one kind of observation in a single table. Let's see the billboard example again after the `gather` operation we did before:

```{r}
tidy_billboard <- billboard %>%
  gather(week, rank, wk1:wk76, na.rm=TRUE)
tidy_billboard
```

Let's sort this table by track to see a problem with this table:

```{r}
tidy_billboard <- tidy_billboard %>%
  arrange(track)
tidy_billboard
```

We have a lot of repeated information in many of these rows (the artist, track name, year, title and date entered). The problem is that this table contains information about both tracks and rank in billboard. That's two different kinds of observations that should belong in two different tables in a tidy dataset.

Let's make a song table that only includes information about songs:

```{r}
song <- tidy_billboard %>%
  select(artist, track, year, time, date.entered) %>%
  unique()
song
```

The `unique` function removes any duplicate rows in a table. That's how we have a single row for each song. 

Next, we would like to remove all the song information from the rank table. But we need to do it in a way that still remembers which song each ranking observation corresponds to. To do that, let's first give each song an identifier that we can use to link songs and rankings. So, we can produce the final version of our song table like this:

```{r}
song <- tidy_billboard %>%
  select(artist, track, year, time, date.entered) %>% 
  unique() %>%
  mutate(song_id = row_number())
song
```

The `mutate` function adds a new column to the table, in this case with column name `song_id` and value the row number the song appears in the table (from the `row_number` column).

Now we can make a rank table, we combine the tidy billboard table with our new song table using a `join` (we'll learn all about joins later). It checks the values on each row of the billboard table and looks for rows in the song table that have the exact same values, and makes a new row that combines the information from both tables. 

```{r}
tidy_billboard %>%
  left_join(song, c("artist", "year", "track", "time", "date.entered"))
```

That adds the `song_id` variable to the `tidy_billboard` table. So now we can remove the song information and only keep ranking information and the `song_id`.

```{r}
rank <- tidy_billboard %>%
  left_join(song, c("artist", "year", "track", "time", "date.entered")) %>%
  select(song_id, week, rank)
rank
```

Challenge:
Let's do a little better job at tidying the billboard dataset:

1. When using `gather` to make the `week` and `rank` columns, remove any weeks where the song does not appear in the top 100. This is coded as missing (`NA`). See the `na.rm` argument to `gather`.  
2. Make `week` a numeric variable (i.e., remove `wk`). See what the `extract_numeric` function does.  
3. Instead of `date.entered` add a `date` column that states the actual date of each ranking. See how R deals with dates `?Date` and how you can turn a string into a `Date` using `as.Date`.  
4. Sort the resulting table by date and rank.
5. Make new `song` and `rank` tables. `song` will now not have the `date.entered` column, and `rank` will have the new `date` column you have just created.

## Data wrangling with `dplyr`

In previous lectures we discussed the `data.frame` to introduced the structure we usually see in a dataset before we start analysis: 

1. Each attribute/variable forms a column
2. Each entity/(observational unit) forms a row
3. Each type of entity/(observation unit) forms a table

Although we did not explicitly mentioned number 3, in more complex datasets we want to make sure we divide different entity types into their respective table. We will discuss this in more detail when we see data models (in the database sense) later on. We will refer to data organized in this fashion as _tidy data_.

In this section we introduce operations and manipulations that commonly arise in analyses. We center our discussion around the idea that we are operating over tidy data, and we want to ensure that the operations we apply also generate tidy data as a result. 

### `dplyr`

We will use the `dplyr` package to introduce these oprations. I think it is one of the most beautiful tools created for data analysis. It clearly defines and efficiently implements most common data manipulation operations (verbs) one comes across in data analysis. It is built around tidy data principles. It also presents uniform treatment of multiple kinds of data sources (in memory files, partially loaded files, databases). It works best when used in conjuction with the non-standard _pipe_ operator (`%>%`) first introduced by the `magrittr` package. 

A complete introduction to `dplyr` is found here: [http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html](http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)

We will use a dataset of inbound and outbound flights to New York City as an example:

```{r}
library(nycflights13)
data(flights)
```

## Single-table manipulation

We will first look at operations that work over a single table at a time. 

Single table verbs:

- `filter()` and `slice()`: subset observations (entities)    
- `arrange()`: sort observations (entities)    
- `select()` and `rename()`: subset variables (attributes)  
- `distinct()`: make entities unique  
- `mutate()` and `transmutate()`: add a new variable (attribute)  
- `summarize()`: compute a summary statistics for one or more variables  
- `sample_n()` and `sample_frac()`: sample observations from a data table   

### Subsetting Observations

The first fundamental operation we learned about early in this course is subsetting, or filtering, observations (entities, rows) in a dataset. Recall that we could subset by a set of indices (say, all even rows, this is used when splitting datasets to train and test statistical models). Much more useful is the ability to filter observations based on attribute values. 

![](img/subset.png)

```{r, eval=FALSE}
# include only flights on United Airlines
flights %>% filter(carrier == "UA")

# select even samples, note function `n` defined by dplyr
flights %>% slice(seq(1, n(), by=2))
```

### Subsetting Variables

Frequently, we may want to restrict a data analysis to a subset of variables (attributes, columns) to improve efficiency or interpretability. 

![](img/select.png)

```{r, eval=FALSE}
# select only month carrier and origin variables
flights %>% select(month, carrier, origin)
```

On large, complex, datasets the ability to perform this selection based on properties of column/attribute names is very powerful. For instance, in the `billboard` dataset we saw in a previous unit, we can select columns using partial string matching:

```{r, eval=FALSE}
billboard %>%
  select(starts_with("wk"))
```

### Creating New Variables

One of the most common operations in data analysis is to create new variables (attributes), based on other existing attributes. 

![](img/mutate.png)


These manipulations are used for transformations of existing single variables, for example, squaring a given varaible (`x -> x^2`), to make visualization or other downstream analysis more effective. In other cases, we may want to compute functions of existing variables to improve analysis or interpretation of a dataset.

Here is an example creating a new variable as a function of two existing variables

```{r, eval=FALSE}
# add new variable with total delay
flights %>% mutate(delay=dep_delay + arr_delay)
```

### Summarizing Data

Much of statistical analysis, modeling and visualization is based on computing summaries (refered to as summary statistics) for variables (attributes), or other data features, of datasets. The `summarize` operation summarizes one variable (columns) over multiple observations (rowss) into a single value.

![](img/summarize.png)

```{r, eval=FALSE}
# compute mean total delay across all flights
flights %>% 
  mutate(delay = dep_delay + arr_delay) %>%
  summarize(mean_delay = mean(delay, na.rm=TRUE),
            min_delay = min(delay, na.rm=TRUE),
            max_delay = max(delay, na.rm=TRUE))
```

### Grouping Data

Aggregation and summarization also go hand in hand with data grouping, where aggregates, or even variable transformations are performed _conditioned_ on other variables. The notion of _conditioning_ is fundamental and we will see it very frequently through the course. It is the basis of statistical analysis and Machine Learning models for regression and prediction, and it is essential in understanding the design of effective visualizations.

![](img/groupby.png)

So the goal is to group observations (rows) with the same value of one or
more variables (columns). In the `dplyr` implementation, the `group_by` function in essence annotates the rows of a data table as belonging to a specific group. When `summarize` is the applied onto this annotated data table, summaries are computed for each group, rather than the whole table.

```{r, eval=FALSE}
# compute mean total delay per carrier
flights %>%
  mutate(delay = dep_delay + arr_delay) %>%
  group_by(carrier) %>%
  summarize(delay=mean(delay, na.rm=TRUE))
```

## Two-table manipulation

We saw above manipulations defined over single tables. In this section we look at efficient methods to combine data from multiple tables. The fundamental operation here is the `join`, which is a workhorse of database system design and impementation. The `join` operation combines rows from two tables to create a new single table, based on matching criteria specified over attributes of each of the two tables. 

Consider the example of joining the `flights` and `airlines` table:

```{r}
head(flights)
head(airlines)
```

Here, we want to add airline information to each flight. We can do so by joining the attributes of the respective airline from the `airlines` table with the `flights` table based on the values of attributes `flights$carrier` and `airlines$carrier`. Specifically, every row of `flights` with a specific value for `flights$carrier`, is joined with the the corresponding row in `airlines` with the same value for `airlines$carrier`. We will see four different ways of performing this operation that differ on how non-matching observations are handled.

### Left Join 

In this case, all observations on left operand (LHS) are retained:

![](img/join_lhs.png)
![](img/left_join.png)

```{r, eval=FALSE}
flights %>%
  left_join(airlines, by="carrier")
```

RHS variables for LHS observations with no matching RHS observations are coded as `NA`.

####  Right Join

All observations on right operand (RHS) are retained:

![](img/join_lhs.png)
![](img/right_join.png)

```{r, eval=FALSE}
flights %>%
  right_join(airlines, by="carrier")
```

LHS variables for RHS observations with no matching LHS observations are coded as `NA`.

#### Inner Join

Only observations matching on both tables are retained

![](img/join_lhs.png)
![](img/inner_join.png)


```{r, eval=FALSE}
flights %>%
  inner_join(airlines, by="carrier")
```



#### Full Join 

All observations are retained, regardless of matching condition

![](img/join_lhs.png)
![](img/full_join.png)

```{r, eval=FALSE}
flights %>%
  full_join(airlines, by="carrier")
```

All values coded as `NA` for non-matching observations as appropriate.

### Join conditions

All join operations are based on a matching condition:

```{r, eval=FALSE}
flights %>%
  left_join(airlines, by="carrier")
```

specifies to join observations where `flights$carrier` equals `airlines$carrier`.


In this case, where no conditions are specified using the `by` argument:

```{r, eval=FALSE}
flights %>%
  left_join(airlines)
```

a *natural join* is perfomed. In this case all variables with the same name in both tables are used in join condition.

You can also specify join conditions on arbitrary attributes using the `by` argument.

```{r, eval=FALSE}
flights %>%
  left_join(airlines, by=c("carrier" = "name"))
```


### Filtering Joins

We've just seen *mutating joins* that create new tables. *Filtering joins* use join conditions to filter a specific table.

```{r}
flights %>% anti_join(airlines, by="carrier")
```

Filters the `flights` table to only include flights from airlines that
*are not* included in the `airlines` table.

## Final note on `dplyr`

- Very efficient implementation of these operations. 
- More info: [http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html](http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)
- Cheatsheet: [http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

## Exercise

1. Clean up the Diego Luna ratings data scraped in the previous unit. 
  - Only keep movies that have a rating and that Diego Luna acts in
  - Convert the rating to a numeric variable (use the `str_replace` and `type_convert` functions)
  
2. Clean up the movie budget data scraped in the previous unit.
  - Remove rows with missing values
  - Make the budget and revenue columns numeric and expressed in millions

3. Join the Diego Luna ratings and movie budget data using the movie title as the join variable

4. Think and implement ways of making the integration of these two datasets more robust.

<!--chapter:end:06-tidying.rmd-->

# Visualizing data

We are now entering an important step of what we would want to do with a dataset before starting modeling using statistics or Machine Learning. We have seen manipulations and operations that prepare datasets into tidy (or normal form), compute summaries, and join tables to obtain organized, clean data tables that contain the observational units, or entities, we want to model statistically. 

At this point, we want to perform _Exploratory Data Analysis_ to better understand the data at hand, and help us make decisions about appropriate statistical or Machine Learning methods, or data transformations that may be helpful to do. Moreover, there are many instances where statistical data modeling is not required to tell a clear and convincing story with data. Many times an effective visualization can lead to convincing conclusions.

### EDA (Exploratory Data Analysis)

The goal of EDA is to perform an initial exploration of attributes/variables across entities/observations. In this section,
we will concentrate on exploration of single or pairs of variables. Later on in the course we will see _dimensionality reduction_ methods that are useful in exploration of more than two variables at a time.

Ultimately, the purpose of EDA is to spot problems in data (as part of data wrangling) and understand variable properties like:

- central trends (mean)
- spread (variance)
- skew
- suggest possible modeling strategies (e.g., probability distributions)
  
We also want to use EDA to understand relationships between pairs of variables, e.g. their correlation or covariance.

## Visualization of single variables

Let's begin by using R's basic graphics capabilities which are great for creating quick plots especially for EDA. We will then see `ggplot2` that requires you to be a bit more thoughtful on data exploration that can lead to good ideas about analysis and modeling.

Let's start with a very simple visualization of the `dep_delay` variable in the flights dataset. 

```{r plot_delay, cache=TRUE}
library(dplyr)
library(nycflights13)

plot(flights$dep_delay)
```

That's not particularly informative since there is no structure to this plot. Let's change the order of the points to depend on departure delay and change the graphical representation to make easier to see.

```{r plot_sorted_delay, cache=TRUE}
plot(sort(flights$dep_delay), type="h", ylab="Departure Delay")
```

What can we make of that plot now? Start thinking of _central tendency_, _spread_ and _skew_ as you look at that plot.

Let's now create a graphical summary of that variable to incorporate observations made from this initial plot. Let's start with a _histogram_: it divides the _range_ of the `dep_delay` variable into **equal-sized** bins, then plots the number of observations within each bin. What additional information does this new visualization gives us about this variable?

```{r hist_delay, cache=TRUE}
hist(flights$dep_delay, xlab="Departure Delay")
```

The `nclass` parameter controls the number of bins into which the `dep_delay` range is divided. Try changing that parameter and see what happens.

Now, we can (conceptually) make the bins as small as possible and get a smooth curve that describes the _distribution_ of values of the `dep_delay` variable. We call this a _density_ plot:

```{r density_delay, cache=TRUE}
plot(density(flights$dep_delay, na.rm=TRUE), xlab="Departure Delay")
```

Now, one more very useful way of succintly graphically summarizing the distribution of a variable is using a `boxplot`. 

```{r boxplot_delay, cache=TRUE}
boxplot(flights$dep_delay, ylab="Departure Delay")
```

That's not very clear to see, so let's do a transformation of this data to see this better:

```{r boxplot_transformed_delay, cache=TRUE}
boxplot(log(flights$dep_delay - 
            min(flights$dep_delay,na.rm=TRUE)
            +1), ylab="Departure Delay")
```

So what does this represent: 
  (a) central tendency (using the median) is represented by the black line within the box, 
  (b) spread (using inter-quartile range) is represented by the box and whiskers. 
  (c) outliers (data that is _unusually_ outside the spread of the data) 
  
We will see more formal descriptions of these summary statistics in the next section, but you can see what we are trying to capture with them graphically.

### Visualization of pairs of variables

Now we can start looking at the relationship between pairs of variables. Suppose we want to see the relationship between `dep_delay`, a _numeric_ variable, and `origin`, a _categorical_ variable. The neat thing here is that we can start thinking about conditioning as we saw before. Here is how we can see a plot of the distribution of departure delays _conditioned_ on origin airport.

```{r delay_by_origin, cache=TRUE}
boxplot(log(flights$dep_delay - min(flights$dep_delay, na.rm=TRUE) + 1) ~ flights$origin,
        ylab="Departure Delay", xlab="Airport of origin")
```

For pairs of continuous variables, the most useful visualization is the scatter plot. This gives an idea of how
one variable varies conditioned on another variable.

```{r delay_scatter, cache=TRUE}
plot(flights$dep_delay, flights$arr_delay, xlab="Departure Delay", ylab="Arrival Delay")
```

## EDA with the grammar of graphics

While we have seen a basic repertoire of graphics it's easier to proceed if we have a bit more formal way of thinking about graphics and plots. Here is where we will use the _grammar of graphics_ implemented in R by the package `ggplot2`.

The central premise is to characterize the building pieces behind plots:

1. The data that goes into a plot, works best when data is tidy
2. The mapping between data and *aesthetic* attributes
3. The *geometric* representation of these attributes

Let's start with a simple example:

```{r}
library(dplyr)
library(ggplot2)
library(Lahman)
batting <- tbl_df(Batting)

# scatter plot of at bats vs. runs for 2010
batting %>% 
  filter(yearID == "2010") %>%
  ggplot(aes(x=AB, y=R)) +
    geom_point()
```

**Data**: Batting table filtering for year  
**Aesthetic attributes**: 
  - x-axis mapped to variables `AB`
  - y-axis mapped to variable `R`  
  
**Geometric Representation**: points!  

Now, you can cleanly distinguish the constituent parts of the plot.
E.g., change the geometric representation

```{r}
# scatter plot of at bats vs. runs for 2010
batting %>% 
  filter(yearID == "2010") %>%
  ggplot(aes(x=AB, y=R, label=teamID)) +
    geom_text() +
    geom_point()
```

E.g., change the data.

```{r}
# scatter plot of at bats vs. runs for 1995
batting %>% 
  filter(yearID == "1995") %>%
  ggplot(aes(x=AB, y=R)) +
    geom_point()
```

E.g., change the aesthetic.

```{r}
# scatter plot of at bats vs. hits for 2010
batting %>% 
  filter(yearID == "2010") %>%
  ggplot(aes(x=AB, y=H)) +
    geom_point()
```

Let's make a line plot 

What do we change? (data, aesthetic or geometry?)

```{r}
batting %>%
  filter(yearID == "2010") %>%
  sample_n(100) %>%
  ggplot(aes(x=AB, y=H)) +
  geom_line()
```

Let's add a regression line 

What do we add? (data, aesthetic or geometry?)

```{r}
batting %>%
  filter(yearID == "2010") %>%
  ggplot(aes(x=AB, y=H)) +
  geom_point() + 
  geom_smooth(method=lm)
```

### Other aesthetics

Using other aesthetics we can incorporate information from other variables.

Color: color by categorical variable

```{r}
batting %>%
  filter(yearID == "2010") %>%
  ggplot(aes(x=AB, y=H, color=lgID)) +
  geom_point() + 
  geom_smooth(method=lm)
```


Size: size by (discrete) numeric variable

```{r}
batting %>%
  filter(yearID == "2010") %>%
  ggplot(aes(x=AB, y=R, size=HR)) +
  geom_point() + 
  geom_smooth(method=lm)
```

### Faceting

The last major component of exploratory analysis called `faceting` in visualization,
corresponds to `conditioning` in statistical modeling, we've seen it as the motivation of `grouping`
when wrangling data.

```{r}
batting %>%
  filter(yearID %in% c("1995", "2000", "2010")) %>%
  ggplot(aes(x=AB, y=R, size=HR)) +
  facet_grid(lgID~yearID) +
  geom_point() + 
  geom_smooth(method=lm)
```

## Exercise

Use `ggplot2` to make a scatter plot of domestic gross revenue vs. Rotten Tomato rating for Diego Luna's movies.


<!--chapter:end:07-visualizing.rmd-->

# Capstone Project 1

## Project 1

- Write a script that includes all steps for
scraping, cleaning, manipulating and making the Diego Luna
movie analysis plot.

- Write a set of functions to perform the same analysis
given an actor's name

## Project 2

Download data from the ENADID and make a plot
of internal vs. external migration patterns in Mexico
from 1992 to 2014. Data for 2009 can be found here:
http://www.beta.inegi.org.mx/proyectos/enchogares/especiales/enadid/2009/default.html. Data for other years is also available there. Write your analysis in an R script.

<!--chapter:end:08-capstone_1.rmd-->

# Introduction to tidy regression analysis

Linear regression is a very elegant, simple, powerful and commonly used technique for data analysis. 

## Simple Regression

Let's start with the simplest linear model. The goal here is to analyze the relationship between a _continuous numerical_ variable $Y$ and another (_numerical_ or _categorical_) variable $X$. We assume that in our population of interest the relationship between the two is given by a linear function:

$$
Y = \beta_0 + \beta_1 X
$$

Here is (simulated) data from an advertising campaign measuring sales and the amount spent in advertising. We think that sales are related to the amount of money spent on TV advertising:

$$
\mathtt{sales} \approx \beta_0 + \beta_1 \times \mathtt{TV}
$$

![](img/regression_example.png)

Given this data, we would say that we _regress_ `sales` on `TV` when we perform this regression analysis. As before, given data we would like to estimate what this relationship is in the _population_ (what is the population in this case?). What do we need to estimate in this case? Values for $\beta_0$ and $\beta_1$. 

Let's take a look at some data. Here is data measuring characteristics of cars, including horsepower, weight, displacement, miles per gallon. Let's see how well a linear model captures the relationship between miles per gallon and weight

```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(dplyr)
library(ggplot2)
library(broom)

data(Auto)

Auto %>%
  ggplot(aes(x=weight, y=mpg)) +
    geom_point() + 
    geom_smooth(method=lm) + 
    theme_minimal()
```

In R, linear models are built using the `lm` function

```{r}
auto_fit <- lm(mpg~weight, data=Auto)
auto_fit
```

This states that for this dataset $\hat{\beta}_0 = `r auto_fit$coef[1]`$ and $\hat{\beta}_1 = `r auto_fit$coef[2]`$. What's the interpretation? According to this model, a weightless car `weight=0` would run $\approx `r round(auto_fit$coef[1], 2)`$ _miles per gallon_ on average, and, on average, a car would run $\approx `r -round(auto_fit$coef[2],3)`$ _miles per gallon_ fewer for every extra _pound_ of weight. Note, that the units of the outcome $Y$ and the predictor $X$ matter for the interpretation of these values.

## Inference

Now that we have an estimate, we want to know how good of an estimate this is. An important point to understand is that like the sample mean, the regression line we learn from a specific dataset is an estimate. A different sample from the same population would give us a different estimate (regression line). 

But, statistical theory tells us that, on average, we are close to population regression line (I.e., close to $\beta_0$ and $\beta_1$), that the spread around $\beta_0$ and $\beta_1$ is well approximated by a normal distribution and that the spread goes to zero as the sample size increases.

![](img/population_line.png)

### Confidence Interval

We can construct a confidence interval to say how precise we think our estimates of the population regression line is. In particular, we want to see how precise our estimate of $\beta_1$ is, since that captures the relationship between the two variables. 

```{r}
auto_fit_stats <- auto_fit %>%
  tidy() %>%
  select(term, estimate, std.error)
auto_fit_stats
```

This `tidy` function is defined by the `broom` package, which is very handy to manipulate the result of learning models in a consistent manner. The `select` call removes some extra information that we will discuss shortly.

```{r, echo=FALSE}
confidence_interval_offset <- 1.95 * auto_fit_stats$std.error[2]
confidence_interval <- round(c(auto_fit_stats$estimate[2] - confidence_interval_offset,
                               auto_fit_stats$estimate[2],
                               auto_fit_stats$estimate[2] + confidence_interval_offset), 4)
```

Given the confidence interval, we would say, "on average, a car runs $_{`r confidence_interval[1]`} `r confidence_interval[2]`_{`r confidence_interval[3]`}$ _miles per gallon_ fewer per pound of weight.

### The $t$-statistic and the $t$-distribution

We can also test a null hypothesis about this relationship: "there is no relationship between weight and miles per gallon", which translates to $\beta_1=0$. According to the statistical theory if this hypothesis is true then the distribution of $\hat{\beta}_1$ is well approximated by $N(0,\mathrm{se}(\hat{\beta}_1))$, and if we observe the estimated $\hat{\beta}_1$ is _too far_ from 0 according to this distribution then we _reject_ the hypothesis.

Now, there is a technicality here that is worth paying attention to. The normal approximation is good as sample size increases, but what about moderate sample sizes (say, less than 100)? The $t$ distribution provides a better approximation of the sampling distribution of these estimates for moderate sample sizes, and it tends to the normal distribution as sample size increases.

The $t$ distribution is commonly used in this testing situation to obtain the probability of rejecting the null hypothesis. It is based on the $t$-statistic

$$
\frac{\hat{\beta}_1}{\mathrm{se}(\hat{\beta}_1)}
$$

You can think of this as a _signal-to-noise_ ratio, or a standardizing transformation on the estimated parameter. Under the null hypothesis, it was shown that the $t$-statistic is well approximated by a $t$-distribution with $n-2$ _degrees of freedom_ (we will get back to _degrees of freedom_ shortly). 

In our example, we get a $t$ statistic and P-value as follows:

```{r}
auto_fit_stats <- auto_fit %>%
  tidy()
auto_fit_stats
```

We would say: "We found a statistically significant relationship between weight and miles per gallon. On average, a car runs $_{`r confidence_interval[1]`} `r confidence_interval[2]`_{`r confidence_interval[3]`}$ _miles per gallon_ fewer per pound of weight ($t$=`r round(auto_fit_stats$statistic[2],2)`, $p$-value < $`r signif(auto_fit_stats$p.value[2],3)`$)."

### Global Fit

Now, notice that we can make _predictions_ based on our regression model, and that prediction should be better than a prediction with a simple average. We can use this comparison as a measure of how good of a job we are doing using our model to fit this data: how much of the variance of $Y$ can we _explain_ with our model. To do this we can calculate _total sum of squares_: 

$$
TSS = \sum_i (y_i - \overline{y})^2
$$

(this is the squared error of a prediction using the sample mean of $Y$)

and the _residual sum of squares_:

$$
RSS = \sum_i (y_i - \hat{y}_i)^2
$$

(which is the squared error of a prediction using the linear model we learned)

The commonly used $R^2$ measure comparse these two quantities:

$$
R^2 = \frac{\mathrm{TSS}-\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
$$

These types of global statistics for the linear model can be obtained using the `glance` function in the `broom` package. In our example

```{r}
auto_fit %>%
  glance() %>%
  select(r.squared, sigma, statistic, df, p.value)
```

We will explain the the columns `statistic`, `df` and `p.value` when we discuss regression using more than a single predictor $X$.

## Some important technicalities

We mentioned above that predictor $X$ could be _numeric_ or _categorical_. However, this is not precisely true. We can use a transformation to represent _categorical_ variables. Here is a simple example:

Suppose we have a categorical variable `sex` with values `female` and `male`, and we want to show the relationship between, say `credit card balance` and `sex`. We can create a dummy variable $x$ as follows:

$$
x_i = \left\{
\begin{align}
1 & \textrm{ if female} \\
0 & \textrm{o.w.}
\end{align}
\right.
$$

and fit a model $y = \beta_0 + \beta_1 x$. What is the conditional expectation given by this model? If the person is male, then $y=\beta_0$, if the person is female, then $y=\beta_0 + \beta_1$. So, what is the interpretation of $\beta_1$? The average difference in credit card balance between females and males.

We could do a different encoding:

$$
x_i = \left\{
\begin{align}
+1 & \textrm{ if female} \\
-1 & \textrm{o.w.}
\end{align}
\right.
$$

Then what is the interpretation of $\beta_1$ in this case?

Note, that when we call the `lm(y~x)` function and `x` is a factor with two levels, the first transformation is used by default. What if there are more than 2 levels? We need multiple regression, which we will see shortly.

## Issues with linear regression

There are some assumptions underlying the inferences and predictions we make using linear regression that we should verify are met when we use this framework. Let's start with four important ones that apply to simple regression

### Non-linearity of outcome-predictor relationship

What if the underlying relationship is not linear? We will see later that we can capture non-linear relationships between variables, but for now, let's concentrate on detecting if a linear relationship is a good approximation. We can use exploratory visual analysis to do this for now by plotting residuals $(y_i - \hat{y}_i)^2$ as a function of the fitted values $\hat{y}_i$. 

The `broom` package uses the `augment` function to help with this task. It augments the input data used to learn the linear model with information of the fitted model for each observation

```{r}
augmented_auto <- auto_fit %>%
  augment()
augmented_auto %>% head()
```

With that we can make the plot we need to check for possible non-linearity

```{r}
augmented_auto %>%
  ggplot(aes(x=.fitted,y=.resid)) +
    geom_point() + 
    geom_smooth() +
    labs(x="fitted", y="residual")
```

### Correlated Error

For our inferences to be valid, we need residuals to be independent and identically distributed. We can spot non independence if we observe a trend in residuals as a function of the predictor $X$. Here is a simulation to demonstrate this:

![](img/correlated_error.png)

In this case, our standard error estimates would be underestimated and our confidence intervals and hypothesis testing results would be biased.

### Non-constant variance

Another violation of the iid assumption would be observed if the spread of residuals is not independent of the fitted values. Here is an illustration, and a possible fix using a log transformation on the outcome $Y$.

![](img/residual_variance.png)

## Multivariate Regression

Now that we've seen regression using a single predictor we'll move on to regression using multiple predictors.
In this case, we use models of conditional expectation represented as linear functions of multiple variables.

In the case of our advertising example, this would be a model:

$$
\mathtt{sales} = \beta_0 + \beta_1 \times \mathtt{TV} + \beta_2 \times \mathtt{newspaper} + \beta_3 \times \mathtt{facebook}
$$

These models let us make statements of the type: "holding everything else constant, sales increased on average by 1000 per dollar spent on Facebook advertising" (this would be given by parameter $\beta_3$ in the example model).

### Estimation in multivariate regression

Continuing with our Auto example, we can build a model for miles per gallon using multiple predictors:

```{r, echo=FALSE, message=FALSE}
library(ISLR)
data(Auto)

library(dplyr)
library(broom)
library(ggplot2)
```

```{r}
auto_fit <- lm(mpg~1+weight+cylinders+horsepower+displacement+year, data=Auto)
auto_fit
```

From this model we can make the statement: "Holding everything else constant, cars run 0.76 miles per gallon more each year on average".

### Statistical statements (cont'd)

Like simple linear regression, we can construct confidence intervals, and test a null hypothesis of no relationship ($\beta_j=0$) for the parameter corresponding to each predictor. This is again nicely managed by the `broom` package:

```{r}
auto_fit_stats <- auto_fit %>%
  tidy()
auto_fit_stats %>% knitr::kable()
```

```{r, echo=FALSE}

print_confint <- function(fit_df, term, digits=2) {
  i <- match(term, fit_df$term)
  confint_offset <- 1.95 * fit_df$std.error[i]
  confint <- round(c(fit_df$estimate[i] - confint_offset,
                     fit_df$estimate[i],
                     fit_df$estimate[i] + confint_offset), digits)
  paste0("{}_{", confint[1], "} ", confint[2], "_{", confint[3], "}")
}

print_pval <- function(fit_df, term) {
  i <- match(term, fit_df$term)
  pval <- fit_df$p.value[i]
  out <- ifelse(pval<1e-16, "<1e-16", paste0("=", signif(pval,4)))
  out
}
```

In this case we would reject the null hypothesis of no relationship only for predictors `weight` and `year`. We would write the statement for year as follows:

"Holding everything else constant, cars run $`r auto_fit_stats %>% print_confint("year")`$ miles per gallon more each year on average (P-value$`r auto_fit_stats %>% print_pval("year")`$)".

### The F-test

We can make additional statements for multivariate regression: "is there a relationship between _any_ of the predictors and the response?". Mathematically, we write this as $\beta_1 = \beta_2 = \cdots = \beta_p = 0$.

Under the null, our model for $y$ would be estimated by the sample mean $\overline{y}$, and the error for that estimate is by total sum of squared error $TSS$. As before, we can compare this to the residual sum of squared error $RSS$ using the $F$ statistic:

$$
\frac{(\mathrm{TSS}-\mathrm{RSS})/p}{\mathrm{RSS}/(n-p-1)}
$$

If this statistic is greater (enough) than 1, then we reject hypothesis that there is no relationship between response and predictors. 

Back to our example, we use the `glance` function to compute this type of summary:

```{r}
auto_fit %>% 
  glance() %>%
  select(r.squared, sigma, statistic, df, p.value) %>%
  knitr::kable()
```

In comparison with the linear model only using `weight`, this multivariate model explains _more of the variance_ of `mpg`, but using more predictors. This is where the notion of _degrees of freedom_ comes in: we now have a model with expanded _representational_ ability. 

However, the bigger the model, we are conditioning more and more, and intuitively, given a fixed dataset, have fewer data points to estimate conditional expectation for each value of the predictors. That means, that are estimated conditional expectation is less _precise_.

To capture this phenomenon, we want statistics that tradeoff how well the model fits the data, and the "complexity" of the model. Now, we can look at the full output of the `glance` function:

```{r}
auto_fit %>%
  glance() %>%
  knitr::kable()
```

Columns `AIC` and `BIC` display statistics that penalize model fit with model size. The smaller this value, the better. Let's now compare a model only using `weight`, a model only using `weight` and `year` and the full multiple regression model we saw before.

```{r}
lm(mpg~weight, data=Auto) %>%
  glance() %>%
  knitr::kable()
```

```{r}
lm(mpg~weight+year, data=Auto) %>%
  glance() %>%
  knitr::kable()
```

In this case, using more predictors beyond `weight` and `year` doesn't help.

### Categorical predictors (cont'd)

We saw transformations for categorical predictors with only two values, and deferred our discussion of categorical predictors with more than two values. In our example we have the `origin` predictor, corresponding to where the car was manufactured, which has multiple values 

```{r}
Auto <- Auto %>%
  mutate(origin=factor(origin))
levels(Auto$origin)
```

As before, we can only use numerical predictors in linear regression models. The most common way of doing this is to create new dummy predictors to _encode_ the value of the categorical predictor. Let's take a categorical variable `major` that can take values `CS`, `MATH`, `BUS`. We can encode these values using variables $x_1$ and $x_2$ 

$$
x_1 = \left\{
\begin{align}
1 & \textrm{ if MATH} \\
0 & \textrm{ o.w.}
\end{align}
\right.
$$

$$
x_2 = \left\{
\begin{align}
1 & \textrm{ if BUS} \\
0 & \textrm{ o.w.}
\end{align}
\right.
$$

Now let's build a model to capture the relationship between `salary` and `major`: 

$$
\mathtt{salary} = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

What is the expected salary for a CS major? $\beta_0$.  
For a MATH major? $\beta_0 + \beta_1$.
For a BUS major? $\beta_0 + \beta_2$.

So, $\beta_1$ is the average difference in salary between MATH and CS majors.
How can we calculate the average difference in salary between MATH and BUS majors? 
$\beta_1 - \beta_2$.

The `lm` function in R does this transformation by default when a variable has class `factor`.
We can see what the underlying numerical predictors look like by using the `model_matrix` function and passing it the model formula we build:

```{r}
extended_df <- model.matrix(~factor(origin), data=Auto) %>% 
  as.data.frame() %>%
  mutate(origin = factor(Auto$origin))
```

```{r}
extended_df %>%
  filter(origin == "1") %>% head()
```

```{r}
extended_df %>% 
  filter(origin == "2") %>% head()
```

```{r}
extended_df %>%
  filter(origin == "3") %>% head()
```

## Interactions in linear models

The linear models so far include _additive_ terms for a single predictor. That let us made statemnts of the type "holding everything else constant...". But what if we think that a pair of predictors _together_ have a relationship with the outcome. We can add these _interaction_ terms to our linear models as products:
  
  Consider the advertising example:
  
  $$
  \mathtt{sales} = \beta_0 + \beta_1 \times \mathtt{TV} + \beta_2 \times \mathtt{facebook} + \beta_3 \times (\mathtt{TV} \times \mathtt{facebook})
$$
  
  If $\beta_3$ is positive, then the effect of increasing TV advertising money is increased if facebook advertising is also increased.

When using categorical variables, interactions have an elegant interpretation. Consider our car example, and suppose we build a model with an interaction between `weight` and `origin`. Let's look at what the numerical predictors look like:

```{r}
Auto$origin <- factor(Auto$origin)
extended_df <- model.matrix(~weight+origin+weight:origin, data=Auto) %>%
as.data.frame() %>%
mutate(origin = factor(Auto$origin))

extended_df %>%
filter(origin == "1") %>% head()
```

```{r}
extended_df %>%
filter(origin == "2") %>% head()
```

```{r}
extended_df %>%
filter(origin == "3") %>% head()
```

So what is the expected miles per gallon for a car with `origin == 1` as a function of weight?

$$
\mathtt{mpg} = \beta_0 + \beta_1 \times \mathtt{weight}
$$

Now how about a car with `origin == 2`?

$$
\mathtt{mpg} = \beta_0 + \beta_1 \times \mathtt{weight} + \beta_2 + \beta_4 \times \mathtt{weight}
$$

Now think of the graphical representation of these lines. For `origin == 1` the intercept of the regression line is $\beta_0$ and its slope is $\beta_1$. For `origin == 2` the intercept
of the regression line is $\beta_0 + \beta_2$ and its slope is $\beta_1+\beta_4$.

`ggplot` does this when we map a factor variable to a aesthetic, say color, and use the `geom_smooth` method:

```{r}
Auto %>%
ggplot(aes(x=weight, y=mpg, color=origin)) +
geom_point() +
geom_smooth(method=lm)
```

The intercept of the three lines seem to be different, but the slope of `origin == 3` looks different (decreases faster) than the slopes of `origin == 1` and `origin == 2` that look very similar to each other. 

Let's fit the model and see how much statistical confidence we can give to those observations:
  
```{r}
auto_fit <- lm(mpg~weight*origin, data=Auto)
auto_fit_stats <- auto_fit %>%
  tidy() 
auto_fit_stats %>% knitr::kable()
```

So we can say that for `origin == 3` the relationship between `mpg` and `weight` is different but not for the other two values of `origin`. Now, there is still an issue here because this could be the result of a poor fit from a linear model, it seems none of these lines do a very good job of modeling the data we have. We can again check this for this model:
  
```{r}
auto_fit %>% 
  augment() %>%
  ggplot(aes(x=.fitted, y=.resid)) +
  geom_point()
```

The fact that residuals are not centered around zero suggests that a linear fit does not work well in this case.

## Additional issues with linear regression

We saw previously some issues with linear regression that we should take into account when using this method for modeling. Multiple linear regression introduces an additional issue that is extremely important to consider when interpreting the results of these analyses: collinearity.

![](img/collinearity.png)

In this example, you have two predictors that are very closely related. In that case, the set of $\beta$'s that minimize RSS may not be unique, and therefore our interpretation is invalid. You can identify this potential problem by regressing predictors onto each other. The usual solution is to fit models only including one of the colinear variables.

## Exercise

Here you will practice and experiment with linear regression using
data from [gapminder.org](http://gapminder.org). I recommend spending a little time looking at material there, it is quite an informative site.

We will use a subset of data provided by gapminder provided by [Jennifer Bryan](http://www.stat.ubc.ca/~jenny/) described in it's [github page](https://github.com/jennybc/gapminder).

The following commands load the dataset

```{r}
library(gapminder)
data(gapminder)
```

For this exercise you will explore how life expectancy has changed over 50 years across the world, and how economic measures like gross domestic product (GDP) are related to it.


**Exercise 1**: _Make a scatter plot of life expectancy across time._

**Question 1**: _Is there a general trend (e.g., increasing or decreasing) for life expectancy across time? Is this trend linear? (answering this qualitatively from the plot, you will do a statistical analysis of this question shortly)_

A slightly different way of making the same plot is looking at the distribution of life expectancy across countries as it changes over time:

```{r, fig.width=12}
library(dplyr)
library(ggplot2)

gapminder %>%
  ggplot(aes(x=factor(year), y=lifeExp)) +
    geom_violin() +
    labs(title="Life expectancy over time",
         x = "year",
         y = "life expectancy")
```

This type of plot is called a _violin plot_, and it displays the distribution of the variable in the y-axis for each value of the variable in the x-axis.

**Question 2**: _How would you describe the distribution of life expectancy across countries for individual years? Is it skewed, or not? Unimodal or not? Symmetric around it's center?_

Based on this plot, consider the following questions.

**Question 3**: _Suppose I fit a linear regression model of life expectancy vs. year (treating it as a continuous variable), and test for a relationship between year and life expectancy, will you reject the null hypothesis of no relationship? (do this without fitting the model yet. I am testing your intuition.)_

**Question 4**: _What would a violin plot of residuals from the linear model in Question 3 vs. year look like? (Again, don't do the analysis yet, answer this intuitively)_

**Question 5**: _According to the assumptions of the linear regression model, what **should** that violin plot look like?_

**Exercise 2**: _Fit a linear regression model using the `lm` function for life expectancy vs. year (as a continuous variable). Use the `broom::tidy` to look at the resulting model._

**Question 6**: _On average, by how much does life expectancy increase every year around the world?_

**Question 7**: _Do you reject the null hypothesis of no relationship between year and life expectancy? Why?_

**Exercise 3**: _Make a violin plot of residuals vs. year for the linear model from Exercise 2 (use the `broom::augment` function)._

**Question 8**: _Does the plot of Excersize 3 match your expectations (as you answered Question 4)?_

**Exercise 4**: _Make a boxplot (or violin plot) of model residuals vs. continent._


**Question 9**: _Is there a dependence between model residual and continent? If so, what would that suggest when performing a regression analysis of life expectancy across time?_

**Exercise 5**: _Use `geom_smooth(method=lm)` in ggplot as part of a scatter plot of life expectancy vs. year, grouped by continent (e.g., using the `color` aesthetic mapping)._


**Question 10**: _Based on this plot, should your regression model include an interaction term for continent **and** year? Why?_

**Exercise 6**: _Fit a linear regression model for life expectancy including a term for an interaction between continent and year. Use the `broom::tidy` function to show the resulting model._


**Question 11**: _Are all parameters in the model significantly different from zero? If not, which are not significantly different from zero?_

**Question 12**: _On average, by how much does life expectancy increase each year for each  continent? (Provide code to answer this question by extracting relevant estimates from model fit)_

**Exercise 7**: _Use the `anova` function to perform an F-test that compares how well two models fit your data: (a) the linear regression models from Exercise 2 (only including year as a covariate) and (b) Exercise 6 (including interaction between year and continent)._

**Question 13**: _Is the interaction model significantly better than the year-only model? Why?_

**Exercise 8**: _Make a residuals vs. year violin plot for the interaction model. Comment on how well it matches assumptions of the linear regression model. Do the same for a residuals vs. fitted values model._ (You should use the `broom::augment` function).







<!--chapter:end:09-regression.rmd-->

# Text analysis

In this section we will learn about text analysis using R and the tidy data framework. We will be using the `tidytext` package, and material from the excellent textbook "Text Mining with R": http://tidytextmining.com/tidytext.html. Specifically, we will work through the first two chapters of the book "The tidy text format" and
"Sentiment analysis with tidy data". You can use the notes from the link above.


<!--chapter:end:10-text_analysis.rmd-->

# Capstone 2 Project

- Use the `kmeans` function to cluster movies by rating and domestic gross revenue. Use the `broom` package
to tidy the result of using `kmeans` and incorprate this
into your analysis script

- Use ggplot2 to make the domestic gross vs. rating scatter plot. Use color to encode the result of the kmeans algorithm.

- Annotate the plot with movie titles

- Add a regression analysis of revenue vs. rating to address the question "do movies with high ratings tend to have bigger revenues"?


<!--chapter:end:11-capstone_2.rmd-->

# Publishing Analyses with R

Rstudio has created an impressive eco-system for publishing
with R. It has concentrated around two systems: Rmarkdown for 
creating documents that include both text and data analysis code, publishable in multiple formats, and shiny, a framework for creating interactive html applications that allow users to explore
data analyses.

## RMarkdown

Rstudio has provided substantial tutorials and documentation
for Rmarkdown: http://rmarkdown.rstudio.com/

We will go over parts of the tutorial included there: http://rmarkdown.rstudio.com/lesson-1.html

and create an HTML report of the movie analysis we have been
working on.

## Shiny

Like RMarkdown, Rstudio provides great tutorials and documentation
for shiny: https://shiny.rstudio.com/

We will go over parts of the shiny tutorial: https://shiny.rstudio.com/tutorial/lesson1/

We will create an application for our movie analysis as an example.


<!--chapter:end:12-publishing.rmd-->

# Teaching with R

The publishing and sharing capabilities provided by R
and Rstudio are outstanding resources for teaching with
and about R.

## Course materials

Using Rmarkdown as the main authoring tool for course
materials has several advantages:

- Text discussing analyses, including plots and results,
exists in the same document containing code to perform analysis.

- Analyses are reproducible since documents are executable

- Students can interact with teaching materials by downloading
and re-executing or modifying analysis code

- Can create lecture notes and slides, reusing code and text

- Can use bookdown https://bookdown.org/ to create lecture note
collection publishable in different formats 

- Placing materials in github https://github.com makes it possible
for students to download and modify materials. Can also incorporate
feedback and modifications from others.

## Assignments

- Students can submit Rmarkdown (or rendered Rmarkdown) for
assignments to include both code used in analysis and text
discussing analyses.

- Assignments can be handed out in Rmarkdown asking both conceptual and analysis questions. Students can write text 
(with mathematical notation if necessary) for the former, 
and R code for the latter.

## Free and open source

- R and Rstudio are free and open source and remain so after students are done with courses. 

- Many commercial products are free to use for student use, but must be purchased after students are done. 

- This can be limiting for many students, especially those working in subject areas where purchasing expensive software is not a priority

## Wide range of activities

- Besides allowing students to interact with code, using shiny as a teaching
tool can be very powerful.

- Students often find creating interactive applications rewarding

- Wide range of analysis tools, covering many subject areas, or covering
many technologies, allow students to carry out activities they find
specifically motivating and interesting.


<!--chapter:end:13-teaching.rmd-->

# Day 3 Capstone Project

Complete and publish a shiny application for our movie analysis.


<!--chapter:end:14-capstone.rmd-->


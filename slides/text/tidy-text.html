<!DOCTYPE html>
<html>
  <head>
    <title>Ingesting Data</title>
    <meta charset="utf-8">
    <meta name="author" content="Hector Corrada Bravo" />
    <link href="libs/remark-css/example.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Ingesting Data
### Hector Corrada Bravo
### 2017/5/31

---


# The tidy text format 




Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham, tidy data has a specific structure:

* Each variable is a column
* Each observation is a row
* Each type of observational unit is a table
---

## Tidy text

We thus define the tidy text format as being **a table with one-token-per-row.** 

A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. 

For tidy text mining, the **token** that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. 

In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format.

---

## Tidy text

Tidy data sets allow manipulation with a standard set of "tidy" tools, including popular packages such as dplyr, tidyr, ggplot2, and broom. 

By keeping the input and output in tidy tables, users can transition fluidly between these packages. We've found these tidy tools extend naturally to many text analyses and explorations. 
---

## Contrasting tidy text with other data structures

* **String**: Text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.
* **Corpus**: These types of objects typically contain raw strings annotated with additional metadata and details.
* **Document-term matrix**: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf.

---


## The `unnest_tokens` function

Emily Dickinson wrote some lovely text in her time.


```r
text &lt;- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text
```

```
## [1] "Because I could not stop for Death -"   "He kindly stopped for me -"            
## [3] "The Carriage held but just Ourselves -" "and Immortality"
```

---

## The `unnest_tokens` function

This is a typical character vector that we might want to analyze. In order to turn it into a tidy text dataset, we first need to put it into a data frame.


```r
library(dplyr)
text_df &lt;- data_frame(line = 1:4, text = text)

text_df
```

```
## # A tibble: 4 x 2
##    line                                   text
##   &lt;int&gt;                                  &lt;chr&gt;
## 1     1   Because I could not stop for Death -
## 2     2             He kindly stopped for me -
## 3     3 The Carriage held but just Ourselves -
## 4     4                        and Immortality
```

---

## The `unnest_tokens` function


```
## # A tibble: 4 x 2
##    line                                   text
##   &lt;int&gt;                                  &lt;chr&gt;
## 1     1   Because I could not stop for Death -
## 2     2             He kindly stopped for me -
## 3     3 The Carriage held but just Ourselves -
## 4     4                        and Immortality
```

Notice that this data frame containing text isn't yet compatible with tidy text analysis, though. We can't filter out words or count which occur most frequently, since each row is made up of multiple combined words. We need to convert this so that it has **one-token-per-document-per-row**. 

---

## The `unnest_tokens` function

Within our tidy text framework, we need to both break the text into individual tokens (a process called *tokenization*) *and* transform it to a tidy data structure. To do this, we use tidytext's `unnest_tokens()` function.


```r
library(tidytext)

text_df %&gt;%
  unnest_tokens(word, text)
```

```
## # A tibble: 20 x 2
##     line    word
##    &lt;int&gt;   &lt;chr&gt;
##  1     1 because
##  2     1       i
##  3     1   could
##  4     1     not
##  5     1    stop
##  6     1     for
##  7     1   death
##  8     2      he
##  9     2  kindly
## 10     2 stopped
## # ... with 10 more rows
```

The two basic arguments to `unnest_tokens` used here are column names. First we have the output column name that will be created as the text is unnested into it (`word`, in this case), and then the input column that the text comes from (`text`, in this case). Remember that `text_df` above has a column called `text` that contains the data of interest.

---

## The `unnest_tokens` function

After using `unnest_tokens`, we've split each row so that there is one token (word) in each row of the new data frame; the default tokenization in `unnest_tokens()` is for single words, as shown here. Also notice:

* Other columns, such as the line number each word came from, are retained.
* Punctuation has been stripped.
* By default, `unnest_tokens()` converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the `to_lower = FALSE` argument to turn off this behavior).

---

## The `unnest_tokens` function

Having the text data in this format lets us manipulate, process, and visualize the text using the standard set of tidy tools, namely dplyr, tidyr, and ggplot2. 

---

## Tidying the works of Jane Austen {#tidyausten}

Let's use the text of Jane Austen's 6 completed, published novels from the [janeaustenr](https://cran.r-project.org/package=janeaustenr) package, and transform them into a tidy format. 

The janeaustenr package provides these texts in a one-row-per-line format, where a line is this context is analogous to a literal printed line in a physical book. Letâ€™s start with that, and also use `mutate()` to annotate a `linenumber` quantity to keep track of lines in the original format and a `chapter` (using a regex) to find where all the chapters are.


```r
library(janeaustenr)
library(dplyr)
library(stringr)

original_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %&gt;%
  ungroup()

original_books
```

```
## # A tibble: 73,422 x 4
##                     text                book linenumber chapter
##                    &lt;chr&gt;              &lt;fctr&gt;      &lt;int&gt;   &lt;int&gt;
##  1 SENSE AND SENSIBILITY Sense &amp; Sensibility          1       0
##  2                       Sense &amp; Sensibility          2       0
##  3        by Jane Austen Sense &amp; Sensibility          3       0
##  4                       Sense &amp; Sensibility          4       0
##  5                (1811) Sense &amp; Sensibility          5       0
##  6                       Sense &amp; Sensibility          6       0
##  7                       Sense &amp; Sensibility          7       0
##  8                       Sense &amp; Sensibility          8       0
##  9                       Sense &amp; Sensibility          9       0
## 10             CHAPTER 1 Sense &amp; Sensibility         10       1
## # ... with 73,412 more rows
```

To work with this as a tidy dataset, we need to restructure it in the **one-token-per-row** format, which as we saw earlier is done with the `unnest_tokens()` function.


```r
library(tidytext)
tidy_books &lt;- original_books %&gt;%
  unnest_tokens(word, text)

tidy_books
```

```
## # A tibble: 725,054 x 4
##                   book linenumber chapter        word
##                 &lt;fctr&gt;      &lt;int&gt;   &lt;int&gt;       &lt;chr&gt;
##  1 Sense &amp; Sensibility          1       0       sense
##  2 Sense &amp; Sensibility          1       0         and
##  3 Sense &amp; Sensibility          1       0 sensibility
##  4 Sense &amp; Sensibility          3       0          by
##  5 Sense &amp; Sensibility          3       0        jane
##  6 Sense &amp; Sensibility          3       0      austen
##  7 Sense &amp; Sensibility          5       0        1811
##  8 Sense &amp; Sensibility         10       1     chapter
##  9 Sense &amp; Sensibility         10       1           1
## 10 Sense &amp; Sensibility         13       1         the
## # ... with 725,044 more rows
```

This function uses the [tokenizers](https://github.com/ropensci/tokenizers) package to separate each line of text in the original data frame into tokens. The default tokenizing is for words, but other options include characters, n-grams, sentences, lines, paragraphs, or separation around a regex pattern.

Now that the data is in one-word-per-row format, we can manipulate it with tidy tools like dplyr. Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as "the", "of", "to", and so forth in English. We can remove stop words (kept in the tidytext dataset `stop_words`) with an `anti_join()`.


```r
data(stop_words)

tidy_books &lt;- tidy_books %&gt;%
  anti_join(stop_words)
```

The `stop_words` dataset in the tidytext package contains stop words from three lexicons. We can use them all together, as we have here, or `filter()` to only use one set of stop words if that is more appropriate for a certain analysis.

We can also use dplyr's `count()` to find the most common words in all the books as a whole.


```r
tidy_books %&gt;%
  count(word, sort = TRUE) 
```

```
## # A tibble: 13,914 x 2
##      word     n
##     &lt;chr&gt; &lt;int&gt;
##  1   miss  1855
##  2   time  1337
##  3  fanny   862
##  4   dear   822
##  5   lady   817
##  6    sir   806
##  7    day   797
##  8   emma   787
##  9 sister   727
## 10  house   699
## # ... with 13,904 more rows
```

Because we've been using tidy tools, our word counts are stored in a tidy data frame. This allows us to pipe this directly to the ggplot2 package, for example to create a visualization of the most common words (Figure \@ref(fig:plotcount)).


```r
library(ggplot2)

tidy_books %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 600) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

![The most common words in Jane Austen's novels](tidy-text_files/figure-html/plotcount-1.png)

Note that the `austen_books()` function started us with exactly the text we wanted to analyze, but in other cases we may need to perform cleaning of text data, such as removing copyright headers or formatting. You'll see examples of this kind of pre-processing in the case study chapters, particularly Chapter \@ref(pre-processing-text). 

## The gutenbergr package

Now that we've used the janeaustenr package to explore tidying text, let's introduce the [gutenbergr](https://github.com/ropenscilabs/gutenbergr) package [@R-gutenbergr]. The gutenbergr package provides access to the public domain works from the [Project Gutenberg](https://www.gutenberg.org/) collection. The package includes tools both for downloading books (stripping out the unhelpful header/footer information), and a complete dataset of Project Gutenberg metadata that can be used to find works of interest. In this book, we will mostly use the function `gutenberg_download()` that downloads one or more works from Project Gutenberg by ID, but you can also use other functions to explore metadata, pair Gutenberg ID with title, author, language, etc., or gather information about authors. 


&lt;div class="rmdtip"&gt;
&lt;p&gt;To learn more about gutenbergr, check out the &lt;a href="https://ropensci.org/tutorials/gutenbergr_tutorial.html"&gt;package's tutorial at rOpenSci&lt;/a&gt;, where it is one of rOpenSci's packages for data access.&lt;/p&gt;
&lt;/div&gt;

## Word frequencies

A common task in text mining is to look at word frequencies, just like we have done above for Jane Austen's novels, and to compare frequencies across different texts. We can do this intuitively and smoothly using tidy data principles. We already have Jane Austen's works; let's get two more sets of texts to compare to. First, let's look at some science fiction and fantasy novels by H.G. Wells, who lived in the late 19th and early 20th centuries. Let's get [*The Time Machine*](https://www.gutenberg.org/ebooks/35), [*The War of the Worlds*](https://www.gutenberg.org/ebooks/36), [*The Invisible Man*](https://www.gutenberg.org/ebooks/5230), and [*The Island of Doctor Moreau*](https://www.gutenberg.org/ebooks/159). We can access these works using `gutenberg_download()` and the Project Gutenberg ID numbers for each novel.


```r
library(gutenbergr)

hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159))
```




```r
tidy_hgwells &lt;- hgwells %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)
```

Just for kicks, what are the most common words in these novels of H.G. Wells?


```r
tidy_hgwells %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 11,769 x 2
##      word     n
##     &lt;chr&gt; &lt;int&gt;
##  1   time   454
##  2 people   302
##  3   door   260
##  4  heard   249
##  5  black   232
##  6  stood   229
##  7  white   222
##  8   hand   218
##  9   kemp   213
## 10   eyes   210
## # ... with 11,759 more rows
```

Now let's get some well-known works of the BrontÃ« sisters, whose lives overlapped with Jane Austen's somewhat but who wrote in a rather different style. Let's get [*Jane Eyre*](https://www.gutenberg.org/ebooks/1260), [*Wuthering Heights*](https://www.gutenberg.org/ebooks/768), [*The Tenant of Wildfell Hall*](https://www.gutenberg.org/ebooks/969), [*Villette*](https://www.gutenberg.org/ebooks/9182), and [*Agnes Grey*](https://www.gutenberg.org/ebooks/767). We will again use the Project Gutenberg ID numbers for each novel and access the texts using `gutenberg_download()`.


```r
bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))
```




```r
tidy_bronte &lt;- bronte %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)
```

What are the most common words in these novels of the BrontÃ« sisters?


```r
tidy_bronte %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 23,051 x 2
##      word     n
##     &lt;chr&gt; &lt;int&gt;
##  1   time  1065
##  2   miss   855
##  3    day   827
##  4   hand   768
##  5   eyes   713
##  6  night   647
##  7  heart   638
##  8 looked   602
##  9   door   592
## 10   half   586
## # ... with 23,041 more rows
```

Interesting that "time", "eyes", and "hand" are in the top 10 for both H.G. Wells and the BrontÃ« sisters.

Now, let's calculate the frequency for each word for the works of Jane Austen, the BrontÃ« sisters, and H.G. Wells by binding the data frames together. We can use `spread` and `gather` from tidyr to reshape our dataframe so that it is just what we need for plotting and comparing the three sets of novels.


```r
library(tidyr)

frequency &lt;- bind_rows(mutate(tidy_bronte, author = "BrontÃ« Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %&gt;% 
  mutate(word = str_extract(word, "[a-z']+")) %&gt;%
  count(author, word) %&gt;%
  group_by(author) %&gt;%
  mutate(proportion = n / sum(n)) %&gt;% 
  select(-n) %&gt;% 
  spread(author, proportion) %&gt;% 
  gather(author, proportion, `BrontÃ« Sisters`:`H.G. Wells`)
```

We use `str_extract()` here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (like italics). The tokenizer treated these as words, but we don't want to count "\_any\_" separately from "any" as we saw in our initial data exploration before choosing to use `str_extract()`. 

Now let's plot (Figure \@ref(fig:plotcompare)).


```r
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

![Comparing the word frequencies of Jane Austen, the BrontÃ« sisters, and H.G. Wells](tidy-text_files/figure-html/plotcompare-1.png)

Words that are close to the line in these plots have similar frequencies in both sets of texts, for example, in both Austen and BrontÃ« texts ("miss", "time", "day" at the upper frequency end) or in both Austen and Wells texts ("time", "day", "brother" at the high frequency end). Words that are far from the line are words that are found more in one set of texts than another. For example, in the Austen-BrontÃ« panel, words like "elizabeth", "emma", and "fanny" (all proper nouns) are found in Austen's texts but not much in the BrontÃ« texts, while words like "arthur" and "dog" are found in the BrontÃ« texts but not the Austen texts. In comparing H.G. Wells with Jane Austen, Wells uses words like "beast", "guns", "feet", and "black" that Austen does not, while Austen uses words like "family", "friend", "letter", and "dear" that Wells does not.

Overall, notice in Figure \@ref(fig:plotcompare) that the words in the Austen-BrontÃ« panel are closer to the zero-slope line than in the Austen-Wells panel. Also notice that the words extend to lower frequencies in the Austen-BrontÃ« panel; there is empty space in the Austen-Wells panel at low frequency. These characteristics indicate that Austen and the BrontÃ« sisters use more similar words than Austen and H.G. Wells. Also, we see that not all the words are found in all three sets of texts and there are fewer data points in the panel for Austen and H.G. Wells.

Let's quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the BrontÃ« sisters, and between Austen and Wells?


```r
cor.test(data = frequency[frequency$author == "BrontÃ« Sisters",],
         ~ proportion + `Jane Austen`)
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  proportion and Jane Austen
## t = 119.64, df = 10404, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7527837 0.7689611
## sample estimates:
##       cor 
## 0.7609907
```

```r
cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  proportion and Jane Austen
## t = 36.441, df = 6053, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4032820 0.4446006
## sample estimates:
##      cor 
## 0.424162
```

Just as we saw in the plots, the word frequencies are more correlated between the Austen and BrontÃ« novels than between Austen and H.G. Wells.

## Summary

In this chapter, we explored what we mean by tidy data when it comes to text, and how tidy data principles can be applied to natural language processing. When text is organized in a format with one token per row, tasks like removing stop words or calculating word frequencies are natural applications of familiar operations within the tidy tool ecosystem. The one-token-per-row framework can be extended from single words to n-grams and other meaningful units of text, as well as to many other analysis priorities that we will consider in this book.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
